{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "      <th>col_11</th>\n",
       "      <th>col_12</th>\n",
       "      <th>col_13</th>\n",
       "      <th>col_14</th>\n",
       "      <th>col_15</th>\n",
       "      <th>col_16</th>\n",
       "      <th>col_17</th>\n",
       "      <th>col_18</th>\n",
       "      <th>col_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>-2.507711</td>\n",
       "      <td>-1.380179</td>\n",
       "      <td>-1.731852</td>\n",
       "      <td>2.759644</td>\n",
       "      <td>-1.648487</td>\n",
       "      <td>-0.094561</td>\n",
       "      <td>-0.414940</td>\n",
       "      <td>-0.517523</td>\n",
       "      <td>0.652992</td>\n",
       "      <td>0.166092</td>\n",
       "      <td>0.937923</td>\n",
       "      <td>0.068159</td>\n",
       "      <td>0.698941</td>\n",
       "      <td>-0.494359</td>\n",
       "      <td>-0.926061</td>\n",
       "      <td>-0.307777</td>\n",
       "      <td>0.306507</td>\n",
       "      <td>0.179015</td>\n",
       "      <td>0.282898</td>\n",
       "      <td>-0.128178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>-3.165850</td>\n",
       "      <td>-0.455225</td>\n",
       "      <td>-1.546249</td>\n",
       "      <td>-3.717344</td>\n",
       "      <td>0.150185</td>\n",
       "      <td>2.324727</td>\n",
       "      <td>1.542446</td>\n",
       "      <td>-1.141088</td>\n",
       "      <td>0.542797</td>\n",
       "      <td>-0.405756</td>\n",
       "      <td>1.332337</td>\n",
       "      <td>-1.655105</td>\n",
       "      <td>0.556727</td>\n",
       "      <td>1.519054</td>\n",
       "      <td>-0.140718</td>\n",
       "      <td>-0.255736</td>\n",
       "      <td>0.255354</td>\n",
       "      <td>1.707274</td>\n",
       "      <td>-1.867323</td>\n",
       "      <td>0.278887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>-0.523556</td>\n",
       "      <td>3.138877</td>\n",
       "      <td>-1.062066</td>\n",
       "      <td>2.906435</td>\n",
       "      <td>2.927918</td>\n",
       "      <td>-1.109569</td>\n",
       "      <td>0.538517</td>\n",
       "      <td>-0.348787</td>\n",
       "      <td>1.112268</td>\n",
       "      <td>-0.300068</td>\n",
       "      <td>-0.294329</td>\n",
       "      <td>-0.028526</td>\n",
       "      <td>1.271090</td>\n",
       "      <td>-0.168948</td>\n",
       "      <td>0.122677</td>\n",
       "      <td>0.717923</td>\n",
       "      <td>0.975971</td>\n",
       "      <td>0.555093</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>-0.269678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>-1.137635</td>\n",
       "      <td>2.983856</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>1.364167</td>\n",
       "      <td>-1.985387</td>\n",
       "      <td>-2.438406</td>\n",
       "      <td>-0.133302</td>\n",
       "      <td>-1.299570</td>\n",
       "      <td>-0.680647</td>\n",
       "      <td>-1.369259</td>\n",
       "      <td>-1.373810</td>\n",
       "      <td>0.091342</td>\n",
       "      <td>-1.197139</td>\n",
       "      <td>0.233148</td>\n",
       "      <td>-0.047254</td>\n",
       "      <td>-0.872664</td>\n",
       "      <td>-0.535044</td>\n",
       "      <td>-2.670622</td>\n",
       "      <td>-0.077298</td>\n",
       "      <td>1.637037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6111</th>\n",
       "      <td>-1.975512</td>\n",
       "      <td>-0.143283</td>\n",
       "      <td>-0.386317</td>\n",
       "      <td>-0.201906</td>\n",
       "      <td>0.964193</td>\n",
       "      <td>-3.191712</td>\n",
       "      <td>-0.027806</td>\n",
       "      <td>0.622075</td>\n",
       "      <td>-0.444076</td>\n",
       "      <td>2.199865</td>\n",
       "      <td>-0.109465</td>\n",
       "      <td>0.760795</td>\n",
       "      <td>0.097191</td>\n",
       "      <td>0.138494</td>\n",
       "      <td>-0.960107</td>\n",
       "      <td>1.513340</td>\n",
       "      <td>0.414545</td>\n",
       "      <td>0.121863</td>\n",
       "      <td>2.852718</td>\n",
       "      <td>-0.499749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>-1.766048</td>\n",
       "      <td>-1.171273</td>\n",
       "      <td>-1.416255</td>\n",
       "      <td>4.246000</td>\n",
       "      <td>-1.311990</td>\n",
       "      <td>1.760119</td>\n",
       "      <td>-0.012331</td>\n",
       "      <td>-0.229009</td>\n",
       "      <td>1.608737</td>\n",
       "      <td>-0.218211</td>\n",
       "      <td>0.721106</td>\n",
       "      <td>0.642894</td>\n",
       "      <td>-0.133245</td>\n",
       "      <td>-0.380248</td>\n",
       "      <td>0.180807</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>0.600325</td>\n",
       "      <td>0.222074</td>\n",
       "      <td>-1.084148</td>\n",
       "      <td>-1.538107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>-1.566735</td>\n",
       "      <td>0.378162</td>\n",
       "      <td>0.404641</td>\n",
       "      <td>-1.911543</td>\n",
       "      <td>2.181362</td>\n",
       "      <td>-4.116653</td>\n",
       "      <td>-0.623058</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.520340</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>1.095978</td>\n",
       "      <td>0.605265</td>\n",
       "      <td>0.496711</td>\n",
       "      <td>-1.235390</td>\n",
       "      <td>0.484506</td>\n",
       "      <td>0.873014</td>\n",
       "      <td>-0.147343</td>\n",
       "      <td>0.570402</td>\n",
       "      <td>-0.133447</td>\n",
       "      <td>0.370973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-2.557793</td>\n",
       "      <td>-2.536380</td>\n",
       "      <td>-3.467047</td>\n",
       "      <td>-0.065625</td>\n",
       "      <td>-1.879515</td>\n",
       "      <td>-1.965617</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>-1.222711</td>\n",
       "      <td>-1.577725</td>\n",
       "      <td>-0.283389</td>\n",
       "      <td>0.216007</td>\n",
       "      <td>-2.001660</td>\n",
       "      <td>1.271304</td>\n",
       "      <td>1.007134</td>\n",
       "      <td>1.119911</td>\n",
       "      <td>0.081895</td>\n",
       "      <td>0.811402</td>\n",
       "      <td>0.645666</td>\n",
       "      <td>1.283420</td>\n",
       "      <td>-2.663874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.902300</td>\n",
       "      <td>2.428416</td>\n",
       "      <td>-0.101549</td>\n",
       "      <td>1.859264</td>\n",
       "      <td>-1.199220</td>\n",
       "      <td>-2.104229</td>\n",
       "      <td>-0.154633</td>\n",
       "      <td>-0.895444</td>\n",
       "      <td>-0.797047</td>\n",
       "      <td>-0.927523</td>\n",
       "      <td>0.986585</td>\n",
       "      <td>0.735307</td>\n",
       "      <td>0.421717</td>\n",
       "      <td>-0.183612</td>\n",
       "      <td>-0.965146</td>\n",
       "      <td>-0.813010</td>\n",
       "      <td>0.824310</td>\n",
       "      <td>-1.004212</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>-2.011507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>-1.885595</td>\n",
       "      <td>2.256627</td>\n",
       "      <td>-0.237952</td>\n",
       "      <td>2.372064</td>\n",
       "      <td>1.768143</td>\n",
       "      <td>-2.564471</td>\n",
       "      <td>-0.220129</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.340889</td>\n",
       "      <td>-0.318525</td>\n",
       "      <td>-0.332421</td>\n",
       "      <td>-0.282044</td>\n",
       "      <td>-0.953235</td>\n",
       "      <td>-0.953347</td>\n",
       "      <td>-1.463994</td>\n",
       "      <td>0.078389</td>\n",
       "      <td>0.732307</td>\n",
       "      <td>-1.111285</td>\n",
       "      <td>-1.057964</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         col_0     col_1     col_2     col_3     col_4     col_5     col_6  \\\n",
       "4529 -2.507711 -1.380179 -1.731852  2.759644 -1.648487 -0.094561 -0.414940   \n",
       "2897 -3.165850 -0.455225 -1.546249 -3.717344  0.150185  2.324727  1.542446   \n",
       "6361 -0.523556  3.138877 -1.062066  2.906435  2.927918 -1.109569  0.538517   \n",
       "1654 -1.137635  2.983856  0.112493  1.364167 -1.985387 -2.438406 -0.133302   \n",
       "6111 -1.975512 -0.143283 -0.386317 -0.201906  0.964193 -3.191712 -0.027806   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5191 -1.766048 -1.171273 -1.416255  4.246000 -1.311990  1.760119 -0.012331   \n",
       "5226 -1.566735  0.378162  0.404641 -1.911543  2.181362 -4.116653 -0.623058   \n",
       "5390 -2.557793 -2.536380 -3.467047 -0.065625 -1.879515 -1.965617  0.016775   \n",
       "860   1.902300  2.428416 -0.101549  1.859264 -1.199220 -2.104229 -0.154633   \n",
       "7270 -1.885595  2.256627 -0.237952  2.372064  1.768143 -2.564471 -0.220129   \n",
       "\n",
       "         col_7     col_8     col_9    col_10    col_11    col_12    col_13  \\\n",
       "4529 -0.517523  0.652992  0.166092  0.937923  0.068159  0.698941 -0.494359   \n",
       "2897 -1.141088  0.542797 -0.405756  1.332337 -1.655105  0.556727  1.519054   \n",
       "6361 -0.348787  1.112268 -0.300068 -0.294329 -0.028526  1.271090 -0.168948   \n",
       "1654 -1.299570 -0.680647 -1.369259 -1.373810  0.091342 -1.197139  0.233148   \n",
       "6111  0.622075 -0.444076  2.199865 -0.109465  0.760795  0.097191  0.138494   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5191 -0.229009  1.608737 -0.218211  0.721106  0.642894 -0.133245 -0.380248   \n",
       "5226  0.009050  0.520340  0.907352  1.095978  0.605265  0.496711 -1.235390   \n",
       "5390 -1.222711 -1.577725 -0.283389  0.216007 -2.001660  1.271304  1.007134   \n",
       "860  -0.895444 -0.797047 -0.927523  0.986585  0.735307  0.421717 -0.183612   \n",
       "7270  1.220342  0.340889 -0.318525 -0.332421 -0.282044 -0.953235 -0.953347   \n",
       "\n",
       "        col_14    col_15    col_16    col_17    col_18    col_19  \n",
       "4529 -0.926061 -0.307777  0.306507  0.179015  0.282898 -0.128178  \n",
       "2897 -0.140718 -0.255736  0.255354  1.707274 -1.867323  0.278887  \n",
       "6361  0.122677  0.717923  0.975971  0.555093  0.908571 -0.269678  \n",
       "1654 -0.047254 -0.872664 -0.535044 -2.670622 -0.077298  1.637037  \n",
       "6111 -0.960107  1.513340  0.414545  0.121863  2.852718 -0.499749  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "5191  0.180807  1.067949  0.600325  0.222074 -1.084148 -1.538107  \n",
       "5226  0.484506  0.873014 -0.147343  0.570402 -0.133447  0.370973  \n",
       "5390  1.119911  0.081895  0.811402  0.645666  1.283420 -2.663874  \n",
       "860  -0.965146 -0.813010  0.824310 -1.004212  0.017964 -2.011507  \n",
       "7270 -1.463994  0.078389  0.732307 -1.111285 -1.057964  0.754503  \n",
       "\n",
       "[5000 rows x 20 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_features = 20 #20,50,100,250,500\n",
    "n_informative = int(0.33*n_features) #5%,10%,33%,50%,90%\n",
    "n_samples = int(5000/(1-0.33))+1 #7463#5000\n",
    "\n",
    "X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "#X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative,random_state=4,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6',\n",
       "       'col_16'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapicant \n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "#X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative,random_state=4,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "Inp_db = X.copy(deep=True)\n",
    "Inp_db[\"class\"]=y\n",
    "Inp_db = Inp_db.reset_index()\n",
    "Index_col = \"index\"\n",
    "target_col = \"class\"\n",
    "\n",
    "train_idx,val_idx = train_test_split(Inp_db[\"index\"],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5'], dtype='<U5')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=5000, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "X[\"class\"]=y\n",
    "X = X.reset_index()\n",
    "\n",
    "\n",
    "explainer_type = shap.TreeExplainer\n",
    "# if classification is False it is a Regression problem\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=False)\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "train_idx,val_idx = train_test_split(X[\"index\"].values,test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = X[X[\"index\"].isin(train_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "X_val = X[X[\"index\"].isin(val_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "Y_train =  X[X[\"index\"].isin(train_idx)][\"class\"]\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "subset = selector.get_features()\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "selector = PowerSHAP(\n",
    "    model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,force_convergence=True,\n",
    ")\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = selector._processed_shaps_df\n",
    "len(t[t.p_value<0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_101', 'col_102', 'col_104', 'col_105',\n",
       "       'col_106', 'col_108', 'col_109', 'col_110', 'col_111', 'col_112',\n",
       "       'col_113', 'col_115', 'col_117', 'col_118', 'col_12', 'col_121',\n",
       "       'col_122', 'col_123', 'col_125', 'col_126', 'col_127', 'col_13',\n",
       "       'col_130', 'col_131', 'col_132', 'col_135', 'col_136', 'col_137',\n",
       "       'col_138', 'col_139', 'col_140', 'col_141', 'col_143', 'col_144',\n",
       "       'col_145', 'col_146', 'col_147', 'col_150', 'col_153', 'col_155',\n",
       "       'col_158', 'col_159', 'col_16', 'col_160', 'col_163', 'col_165',\n",
       "       'col_167', 'col_168', 'col_169', 'col_17', 'col_170', 'col_171',\n",
       "       'col_172', 'col_173', 'col_175', 'col_18', 'col_183', 'col_184',\n",
       "       'col_185', 'col_186', 'col_187', 'col_189', 'col_19', 'col_190',\n",
       "       'col_192', 'col_193', 'col_194', 'col_195', 'col_196', 'col_197',\n",
       "       'col_2', 'col_20', 'col_200', 'col_201', 'col_203', 'col_204',\n",
       "       'col_205', 'col_206', 'col_207', 'col_21', 'col_210', 'col_211',\n",
       "       'col_212', 'col_213', 'col_214', 'col_216', 'col_218', 'col_219',\n",
       "       'col_22', 'col_220', 'col_221', 'col_222', 'col_223', 'col_225',\n",
       "       'col_226', 'col_227', 'col_228', 'col_229', 'col_230', 'col_231',\n",
       "       'col_232', 'col_233', 'col_235', 'col_238', 'col_239', 'col_24',\n",
       "       'col_240', 'col_241', 'col_244', 'col_245', 'col_247', 'col_248',\n",
       "       'col_249', 'col_25', 'col_250', 'col_251', 'col_252', 'col_253',\n",
       "       'col_255', 'col_256', 'col_258', 'col_259', 'col_26', 'col_262',\n",
       "       'col_263', 'col_264', 'col_265', 'col_269', 'col_271', 'col_273',\n",
       "       'col_274', 'col_275', 'col_277', 'col_278', 'col_28', 'col_280',\n",
       "       'col_281', 'col_283', 'col_286', 'col_287', 'col_288', 'col_290',\n",
       "       'col_291', 'col_292', 'col_293', 'col_295', 'col_296', 'col_297',\n",
       "       'col_298', 'col_299', 'col_30', 'col_301', 'col_302', 'col_303',\n",
       "       'col_304', 'col_305', 'col_308', 'col_310', 'col_315', 'col_316',\n",
       "       'col_317', 'col_318', 'col_32', 'col_321', 'col_323', 'col_324',\n",
       "       'col_325', 'col_328', 'col_329', 'col_33', 'col_330', 'col_331',\n",
       "       'col_332', 'col_334', 'col_335', 'col_337', 'col_338', 'col_341',\n",
       "       'col_343', 'col_344', 'col_345', 'col_346', 'col_347', 'col_348',\n",
       "       'col_349', 'col_351', 'col_355', 'col_356', 'col_359', 'col_36',\n",
       "       'col_361', 'col_362', 'col_363', 'col_364', 'col_366', 'col_367',\n",
       "       'col_368', 'col_369', 'col_370', 'col_372', 'col_373', 'col_375',\n",
       "       'col_376', 'col_377', 'col_378', 'col_379', 'col_38', 'col_380',\n",
       "       'col_383', 'col_384', 'col_386', 'col_387', 'col_388', 'col_390',\n",
       "       'col_392', 'col_394', 'col_395', 'col_397', 'col_398', 'col_399',\n",
       "       'col_4', 'col_40', 'col_400', 'col_401', 'col_402', 'col_403',\n",
       "       'col_405', 'col_406', 'col_407', 'col_408', 'col_409', 'col_410',\n",
       "       'col_415', 'col_417', 'col_419', 'col_42', 'col_421', 'col_424',\n",
       "       'col_426', 'col_429', 'col_43', 'col_430', 'col_433', 'col_436',\n",
       "       'col_437', 'col_439', 'col_44', 'col_441', 'col_445', 'col_446',\n",
       "       'col_448', 'col_449', 'col_47', 'col_48', 'col_49', 'col_490',\n",
       "       'col_51', 'col_53', 'col_55', 'col_56', 'col_58', 'col_6',\n",
       "       'col_61', 'col_62', 'col_63', 'col_65', 'col_66', 'col_67',\n",
       "       'col_68', 'col_7', 'col_70', 'col_73', 'col_74', 'col_77', 'col_8',\n",
       "       'col_80', 'col_81', 'col_83', 'col_84', 'col_85', 'col_86',\n",
       "       'col_87', 'col_88', 'col_9', 'col_90', 'col_91', 'col_92',\n",
       "       'col_93', 'col_95', 'col_96', 'col_97', 'col_98', 'col_99'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(t[t.p_value<0.01].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One by one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "selector = PowerSHAP(\n",
    "    model = LogisticRegressionCV(max_iter=1000),# CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=200,automatic=False, limit_automatic=10,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_auto = PowerSHAP(\n",
    "    model = LogisticRegressionCV(max_iter=1000),#CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=200,automatic=True, limit_automatic=10,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.fit(X_train, y_train)\n",
    "selector_auto.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = selector_auto._processed_shaps_df\n",
    "t_inf = np.sum(np.isin(X_train.columns.values[:int(0.9*n_features)],t[t.p_value<0.01].index.values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import powerSHAP_statistical_analysis\n",
    "\n",
    "shaps_df = selector._shaps_df\n",
    "found_features = []\n",
    "found_informative_features = []\n",
    "found_noise_features = []\n",
    "\n",
    "processed_shaps_df = powerSHAP_statistical_analysis(\n",
    "                        shaps_df[:37],\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        include_all=True,\n",
    ")\n",
    "processed_shaps_df = processed_shaps_df[processed_shaps_df.index.values!=\"random_uniform_feature\"]\n",
    "\n",
    "for i in (range(1,100)):\n",
    "    \n",
    "    found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<i/100]))\n",
    "    found_informative_features.append(np.sum(np.isin(X_train.columns.values[:int(0.33*n_features)],X_train.columns.values[processed_shaps_df[processed_shaps_df.p_value<i/100].index.values.astype(np.int32)])) )\n",
    "    found_noise_features.append(np.sum(1-np.isin(X_train.columns.values[processed_shaps_df[processed_shaps_df.p_value<i/100].index.values.astype(np.int32)],X_train.columns.values[:int(0.33*n_features)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = powerSHAP_statistical_analysis(\n",
    "                        shaps_df[:27],\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        include_all=False,\n",
    "    )\n",
    "t_inf = np.sum(np.isin(X_train.columns.values[:int(0.33*n_features)],X_train.columns.values[t[t.p_value<0.01].index.values.astype(np.int32)])) \n",
    "t_noise = np.sum(1-np.isin(X_train.columns.values[t[t.p_value<0.01].index.values.astype(np.int32)],X_train.columns.values[:int(0.33*n_features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs=plt.subplots(1,1,figsize=(6,5))\n",
    "axs.plot(range(1,100),np.array(found_features),label=\"All outputted features\",alpha=0.5)\n",
    "axs.plot(range(1,100),np.array(found_informative_features),label=\"Informative features found\")\n",
    "axs.plot(range(1,100),np.repeat([83],100-1),label=\"True Informative features\",color='grey',alpha=0.5)\n",
    "axs.plot([26.99,27],[0,140],color=\"red\",label=\"Automatic mode\",alpha=0.5)\n",
    "axs.plot(range(1,100),np.array(found_noise_features),label=\"Noise features\")\n",
    "axs.set_ylabel(\"Found features\")\n",
    "axs.set_xlabel(\"Powershap Iterations\")\n",
    "axs.legend()\n",
    "#axs.set_ylim([0,140])\n",
    "#axs[2].plot(range(2,200),np.array(found_noise_features))\n",
    "#axs[2].scatter(27,t_noise,color=\"red\")\n",
    "#axs[2].set_ylim([0,110])\n",
    "#axs[2].set_ylabel(\"Found noise features\")\n",
    "#axs[2].set_xlabel(\"Iterations\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.jpg\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.pdf\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.svg\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"selector\",\n",
    "            PowerSHAP(\n",
    "                CatBoostClassifier(n_estimators=250,verbose=False,use_best_model=True), automatic=True, limit_automatic=100,#power_alpha=0.001,power_req_iterations=0.999,\n",
    "                #CatBoostRegressor(n_estimators=250,verbose=False), automatic=True, limit_automatic=100,\n",
    "            ),\n",
    "        ),\n",
    "        (\"catboost\", KNeighborsClassifier()),#(n_estimators=250,verbose=False)),\n",
    "        #(\"catboost\", CatBoostRegressor(n_estimators=250,verbose=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "\n",
    "\n",
    "print(\"Baseline\", accuracy_score(KNeighborsClassifier().fit(X_train, y_train).predict(X_test), y_test))\n",
    "#print(\"Baseline\", r2_score(LinearRegression.fit(X_train, y_train).predict(X_test), y_test))\n",
    "\n",
    "\n",
    "print(\"PowerShap feature selection:\", accuracy_score(pipe.predict(X_test), y_test))\n",
    "#print(\"PowerShap feature selection:\", r2_score(pipe.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline\", accuracy_score(CatBoostClassifier(verbose=False,n_estimators=250).fit(X_train, y_train).predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_shaps_df = pipe[0]._processed_shaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_shaps_df[processed_shaps_df.p_value<0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dict_print(output_dict):\n",
    "    for key in output_dict:\n",
    "        #samples\n",
    "        print(key+\": \")\n",
    "        for key_inner in output_dict[key]:\n",
    "            #features\n",
    "            print(key_inner+\": \")\n",
    "            \n",
    "            #per informative\n",
    "            time_str = \"\"\n",
    "            \n",
    "            i = 0\n",
    "            for avg_time in output_dict[key][key_inner][\"Average time\"]:\n",
    "                \n",
    "                i = i+1\n",
    "                \n",
    "                if i ==len(output_dict[key][key_inner][\"Average time\"]):\n",
    "                    time_str = time_str + str(avg_time)+\"s | \"\n",
    "                else:\n",
    "                    time_str = time_str + str(avg_time)+\"s - \"\n",
    "                  \n",
    "            key_strs = \"\"\n",
    "            for key_inner_inner in output_dict[key][key_inner]: \n",
    "                if key_inner_inner != 'Average time':\n",
    "                    key_strs = key_strs + str(output_dict[key][key_inner][key_inner_inner][\"found_informative_features\"]) + \" (\"+str(output_dict[key][key_inner][key_inner_inner][\"outputted_noise_features\"])+\") / \"+str(output_dict[key][key_inner][key_inner_inner][\"informative_features\"]) + \" | \"\n",
    "                \n",
    "            print(time_str+key_strs)\n",
    "                 \n",
    "\n",
    "def output_dict_to_df(output_dict):\n",
    "    df = pd.DataFrame(columns=[\"n_samples\",\"total_features\",\"informative_features\",\"time\",\"seed\",\"found_informative_features\",\"outputted_noise_features\"])\n",
    "    for samples in output_dict.keys():\n",
    "        for features in output_dict[samples].keys():\n",
    "            i = 0\n",
    "            for informative in output_dict[samples][features].keys():\n",
    "                if informative != 'Average time':\n",
    "                    temp = pd.DataFrame.from_dict(output_dict[samples][features][informative])\n",
    "                    temp[\"n_samples\"]=int(samples)\n",
    "                    temp[\"total_features\"]=int(features)\n",
    "                    temp = temp.reset_index()\n",
    "                    temp = temp.rename(columns={\"index\":\"seed\"})\n",
    "                    temp[\"time\"]=output_dict[samples][features]['Average time'][i]\n",
    "                    i = i+1\n",
    "                    df = df.append(temp) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 50 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "import pprint\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 50#250\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_50_Classification_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 250 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        if hypercube:\n",
    "            output_dict_to_df(output_dict).to_csv(\"estimators_250_Classification_output_df.csv\",index=False)\n",
    "        else:\n",
    "            output_dict_to_df(output_dict).to_csv(\"estimators_250_Classification_output_df_polytone.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [50,100]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            for n_redundant in [10,25,33,50]:\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "                \n",
    "                n_redundant_samples = int((n_features-int(n_informative/100*n_features))*(n_redundant/100))\n",
    "                \n",
    "                print(\"Amount of samples = \"+str(n_samples))\n",
    "                print(\"Total used features = \"+str(n_features))\n",
    "                print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "                print(\"Redundant features: \"+str(n_redundant_samples)+\" (\"+str(n_redundant)+\"%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                found_features = []\n",
    "                found_idx_features = []\n",
    "                times = []\n",
    "                for random_seed in [0,1,2,3,4]:\n",
    "                    print(\"Seed \"+str(random_seed))\n",
    "\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=n_redundant_samples, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                    X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    if regression_bool:\n",
    "                        selector = PowerSHAP(\n",
    "                            model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                            automatic=True\n",
    "                        )\n",
    "                    else:\n",
    "                        selector = PowerSHAP(\n",
    "                            model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                            automatic=True\n",
    "                        )\n",
    "                    selector.fit(X, y)\n",
    "\n",
    "                    times.append(time.time() - start_time)\n",
    "\n",
    "                    processed_shaps_df = selector._processed_shaps_df\n",
    "                    print(50*\"-\")\n",
    "\n",
    "                    found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                    found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "\n",
    "                found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "                found_redundant = [np.sum(np.isin(X.columns.values[int(n_informative/100*n_features):int(n_informative/100*n_features)+n_redundant_samples],f_list)) for f_list in found_idx_features]\n",
    "                found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)+n_redundant_samples])) for f_list in found_idx_features]\n",
    "                print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "                print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found redundant: \"+str(found_redundant) + \"/\"+str(n_redundant_samples))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "                print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "\n",
    "                average_times.append(np.round(times,2))\n",
    "\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "\n",
    "                print(100*\"=\")\n",
    "\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [50]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            for n_redundant in [10,25,33,50]:\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "                \n",
    "                n_redundant_samples = int((n_features-int(n_informative/100*n_features))*(n_redundant/100))\n",
    "                \n",
    "                print(\"Amount of samples = \"+str(n_samples))\n",
    "                print(\"Total used features = \"+str(n_features))\n",
    "                print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "                print(\"Redundant features: \"+str(n_redundant_samples)+\" (\"+str(n_redundant)+\"%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                found_features = []\n",
    "                found_idx_features = []\n",
    "                times = []\n",
    "                for random_seed in [0,1,2,3,4]:\n",
    "                    print(\"Seed \"+str(random_seed))\n",
    "\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=n_redundant_samples, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                    X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    # if classification is False it is a Regression problem\n",
    "                    model = CatBoostClassifier(verbose=0, n_estimators=250)\n",
    "                    selector = BorutaShap(model=model,importance_measure='shap',classification=True)\n",
    "                    selector.fit(X, y,verbose=False)\n",
    "\n",
    "                    times.append(time.time() - start_time)\n",
    "\n",
    "                    subset = selector.Subset()\n",
    "                    print(50*\"-\")\n",
    "\n",
    "                    found_features.append(len(subset.columns))\n",
    "                    found_idx_features.append(subset.columns)\n",
    "\n",
    "                found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "                found_redundant = [np.sum(np.isin(X.columns.values[int(n_informative/100*n_features):int(n_informative/100*n_features)+n_redundant_samples],f_list)) for f_list in found_idx_features]\n",
    "                found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)+n_redundant_samples])) for f_list in found_idx_features]\n",
    "                print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "                print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found redundant: \"+str(found_redundant) + \"/\"+str(n_redundant_samples))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "                print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "\n",
    "                average_times.append(np.round(times,2))\n",
    "\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "\n",
    "                print(100*\"=\")\n",
    "\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 500 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 500\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        output_dict_to_df(output_dict).to_csv(\"estimators_500_Classification_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 50 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 50\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_50_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 250 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 250\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_250_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 500 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 500\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_500_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,hypercube=hypercube,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                selector = PowerSHAP(\n",
    "                    model = LogisticRegressionCV(max_iter=1000),\n",
    "                    automatic=True\n",
    "                )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        if hypercube:\n",
    "            output_dict_to_df(output_dict).to_csv(\"logisticregressioncv_output_df.csv\",index=False)\n",
    "        else:\n",
    "            output_dict_to_df(output_dict).to_csv(\"logisticregressioncv_output_df_polytone.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                selector = PowerSHAP(\n",
    "                    model = RandomForestClassifier(),\n",
    "                    automatic=True\n",
    "                )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"randomforest_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopped simulation at 5000 samples, 20000 samples takes on average 300s per model! so at least 3000s per seed. \n",
    "output_dict_to_df(output_dict).to_csv(\"randomforest_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BorutaShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # if classification is False it is a Regression problem\n",
    "                model = CatBoostClassifier(verbose=0, n_estimators=250)\n",
    "                selector = BorutaShap(model=model,importance_measure='shap',classification=True)\n",
    "\n",
    "                selector.fit(X=X, y=y, verbose=False)\n",
    "                subset = selector.Subset()\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(subset.columns))\n",
    "                found_idx_features.append(subset.columns)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "            \n",
    "        output_dict_to_df(output_dict).to_csv(\"250_est_Catboost_borutashap_output_df.csv\",index=False)\n",
    "        \n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'powershap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f40331b29d77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#sys.path.append(\"../powershap\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpowershap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPowerSHAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshapicant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'powershap'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "import shapicant\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:#1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,100,250,500]:#[20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "                X[\"class\"]=y\n",
    "                X = X.reset_index()\n",
    "\n",
    "\n",
    "                explainer_type = shap.TreeExplainer\n",
    "                # if classification is False it is a Regression problem\n",
    "                model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=False)\n",
    "                selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "                train_idx,val_idx = train_test_split(X[\"index\"].values,test_size=0.2,random_state = 0)\n",
    "\n",
    "                X_train = X[X[\"index\"].isin(train_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "                X_val = X[X[\"index\"].isin(val_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "                Y_train =  X[X[\"index\"].isin(train_idx)][\"class\"]\n",
    "\n",
    "                # Run the feature selection\n",
    "                # If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "                # We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "                selector.fit(X_train, Y_train, X_validation=X_val)\n",
    "\n",
    "                subset = selector.get_features()\n",
    "                p_values = selector.p_values_\n",
    "\n",
    "                np.array(subset)\n",
    "\n",
    "                times.append(time.time() - start_time)\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(subset))\n",
    "                found_idx_features.append(subset)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "            \n",
    "        output_dict_to_df(output_dict).to_csv(\"shapicant_output_df.csv\",index=False)\n",
    "        \n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4454adb76e1590f06b0cd848f1b87f82b41871aa5648994c4db0c9fbdc1ef435"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
