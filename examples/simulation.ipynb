{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>col_490</th>\n",
       "      <th>col_491</th>\n",
       "      <th>col_492</th>\n",
       "      <th>col_493</th>\n",
       "      <th>col_494</th>\n",
       "      <th>col_495</th>\n",
       "      <th>col_496</th>\n",
       "      <th>col_497</th>\n",
       "      <th>col_498</th>\n",
       "      <th>col_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>16.745219</td>\n",
       "      <td>3.064825</td>\n",
       "      <td>-2.863545</td>\n",
       "      <td>-13.510380</td>\n",
       "      <td>5.315558</td>\n",
       "      <td>8.874887</td>\n",
       "      <td>-0.422799</td>\n",
       "      <td>-11.292289</td>\n",
       "      <td>-7.055259</td>\n",
       "      <td>-8.252094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.188208</td>\n",
       "      <td>-0.378725</td>\n",
       "      <td>-0.397908</td>\n",
       "      <td>-0.136526</td>\n",
       "      <td>-0.958061</td>\n",
       "      <td>-0.630357</td>\n",
       "      <td>-0.159185</td>\n",
       "      <td>-0.620803</td>\n",
       "      <td>-1.010112</td>\n",
       "      <td>-1.465302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>10.602683</td>\n",
       "      <td>6.263638</td>\n",
       "      <td>5.030698</td>\n",
       "      <td>1.275159</td>\n",
       "      <td>-4.274334</td>\n",
       "      <td>5.887773</td>\n",
       "      <td>7.712275</td>\n",
       "      <td>2.122422</td>\n",
       "      <td>12.012534</td>\n",
       "      <td>-12.924060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317056</td>\n",
       "      <td>2.696913</td>\n",
       "      <td>-0.828865</td>\n",
       "      <td>0.567702</td>\n",
       "      <td>0.743005</td>\n",
       "      <td>0.574161</td>\n",
       "      <td>-0.437067</td>\n",
       "      <td>0.293724</td>\n",
       "      <td>1.449087</td>\n",
       "      <td>-1.036390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>-7.341757</td>\n",
       "      <td>-2.613107</td>\n",
       "      <td>-14.257762</td>\n",
       "      <td>10.934357</td>\n",
       "      <td>17.126862</td>\n",
       "      <td>-17.145643</td>\n",
       "      <td>-11.866992</td>\n",
       "      <td>-9.229942</td>\n",
       "      <td>12.799775</td>\n",
       "      <td>-5.462458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365749</td>\n",
       "      <td>-0.317567</td>\n",
       "      <td>-1.250173</td>\n",
       "      <td>-0.201837</td>\n",
       "      <td>0.515585</td>\n",
       "      <td>1.730105</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>0.636158</td>\n",
       "      <td>-1.585005</td>\n",
       "      <td>-0.477964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>15.103830</td>\n",
       "      <td>-19.380835</td>\n",
       "      <td>11.973576</td>\n",
       "      <td>-8.294274</td>\n",
       "      <td>-10.290333</td>\n",
       "      <td>19.368051</td>\n",
       "      <td>-8.909064</td>\n",
       "      <td>-14.724182</td>\n",
       "      <td>8.982827</td>\n",
       "      <td>-18.175523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.938948</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>0.792452</td>\n",
       "      <td>0.354764</td>\n",
       "      <td>0.886241</td>\n",
       "      <td>-0.988053</td>\n",
       "      <td>-1.464783</td>\n",
       "      <td>1.549964</td>\n",
       "      <td>-1.125569</td>\n",
       "      <td>-0.285697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6111</th>\n",
       "      <td>10.456393</td>\n",
       "      <td>-18.788397</td>\n",
       "      <td>-9.642759</td>\n",
       "      <td>-4.692734</td>\n",
       "      <td>6.591294</td>\n",
       "      <td>-21.168312</td>\n",
       "      <td>7.804272</td>\n",
       "      <td>4.631699</td>\n",
       "      <td>-23.114104</td>\n",
       "      <td>-4.831323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.741045</td>\n",
       "      <td>-0.863689</td>\n",
       "      <td>-0.039028</td>\n",
       "      <td>-0.456419</td>\n",
       "      <td>-2.576476</td>\n",
       "      <td>0.189589</td>\n",
       "      <td>0.778050</td>\n",
       "      <td>-0.611166</td>\n",
       "      <td>0.340887</td>\n",
       "      <td>-0.485027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>-21.483905</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>-4.419907</td>\n",
       "      <td>-1.641034</td>\n",
       "      <td>-21.870400</td>\n",
       "      <td>-9.777687</td>\n",
       "      <td>-8.822450</td>\n",
       "      <td>-14.516811</td>\n",
       "      <td>8.961772</td>\n",
       "      <td>-2.359504</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200718</td>\n",
       "      <td>1.372257</td>\n",
       "      <td>1.042186</td>\n",
       "      <td>-0.230238</td>\n",
       "      <td>1.708837</td>\n",
       "      <td>-0.161753</td>\n",
       "      <td>1.598509</td>\n",
       "      <td>0.781025</td>\n",
       "      <td>0.193675</td>\n",
       "      <td>-0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>-11.222781</td>\n",
       "      <td>9.525985</td>\n",
       "      <td>3.708534</td>\n",
       "      <td>-15.338396</td>\n",
       "      <td>2.099114</td>\n",
       "      <td>11.809690</td>\n",
       "      <td>0.452612</td>\n",
       "      <td>16.381729</td>\n",
       "      <td>-22.193395</td>\n",
       "      <td>-6.377645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387828</td>\n",
       "      <td>1.204048</td>\n",
       "      <td>1.135725</td>\n",
       "      <td>1.584429</td>\n",
       "      <td>1.094804</td>\n",
       "      <td>0.770361</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>-0.526397</td>\n",
       "      <td>-0.241824</td>\n",
       "      <td>1.553872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-12.638624</td>\n",
       "      <td>0.459624</td>\n",
       "      <td>-6.357631</td>\n",
       "      <td>-7.727807</td>\n",
       "      <td>3.518887</td>\n",
       "      <td>-34.666695</td>\n",
       "      <td>14.279390</td>\n",
       "      <td>23.694937</td>\n",
       "      <td>7.549388</td>\n",
       "      <td>12.884744</td>\n",
       "      <td>...</td>\n",
       "      <td>1.008474</td>\n",
       "      <td>0.574237</td>\n",
       "      <td>-1.141540</td>\n",
       "      <td>-0.756666</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>-1.494126</td>\n",
       "      <td>-1.153968</td>\n",
       "      <td>-0.772677</td>\n",
       "      <td>-0.527955</td>\n",
       "      <td>-0.477227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>19.233188</td>\n",
       "      <td>-15.913120</td>\n",
       "      <td>-7.948918</td>\n",
       "      <td>-8.218713</td>\n",
       "      <td>-21.708159</td>\n",
       "      <td>-13.608937</td>\n",
       "      <td>19.088807</td>\n",
       "      <td>0.907969</td>\n",
       "      <td>12.777942</td>\n",
       "      <td>-0.773037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>-0.615041</td>\n",
       "      <td>1.538401</td>\n",
       "      <td>-0.869927</td>\n",
       "      <td>1.483099</td>\n",
       "      <td>0.370104</td>\n",
       "      <td>0.253871</td>\n",
       "      <td>0.075206</td>\n",
       "      <td>-0.632390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>4.241680</td>\n",
       "      <td>-3.739862</td>\n",
       "      <td>10.794133</td>\n",
       "      <td>4.803899</td>\n",
       "      <td>37.951017</td>\n",
       "      <td>-2.560877</td>\n",
       "      <td>-2.821552</td>\n",
       "      <td>-4.350437</td>\n",
       "      <td>-3.697195</td>\n",
       "      <td>3.439736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.545044</td>\n",
       "      <td>0.257616</td>\n",
       "      <td>-0.099697</td>\n",
       "      <td>-0.649687</td>\n",
       "      <td>-1.591489</td>\n",
       "      <td>0.365758</td>\n",
       "      <td>0.204888</td>\n",
       "      <td>1.395029</td>\n",
       "      <td>-0.565957</td>\n",
       "      <td>1.040141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          col_0      col_1      col_2      col_3      col_4      col_5  \\\n",
       "4529  16.745219   3.064825  -2.863545 -13.510380   5.315558   8.874887   \n",
       "2897  10.602683   6.263638   5.030698   1.275159  -4.274334   5.887773   \n",
       "6361  -7.341757  -2.613107 -14.257762  10.934357  17.126862 -17.145643   \n",
       "1654  15.103830 -19.380835  11.973576  -8.294274 -10.290333  19.368051   \n",
       "6111  10.456393 -18.788397  -9.642759  -4.692734   6.591294 -21.168312   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "5191 -21.483905   0.999483  -4.419907  -1.641034 -21.870400  -9.777687   \n",
       "5226 -11.222781   9.525985   3.708534 -15.338396   2.099114  11.809690   \n",
       "5390 -12.638624   0.459624  -6.357631  -7.727807   3.518887 -34.666695   \n",
       "860   19.233188 -15.913120  -7.948918  -8.218713 -21.708159 -13.608937   \n",
       "7270   4.241680  -3.739862  10.794133   4.803899  37.951017  -2.560877   \n",
       "\n",
       "          col_6      col_7      col_8      col_9  ...   col_490   col_491  \\\n",
       "4529  -0.422799 -11.292289  -7.055259  -8.252094  ...  1.188208 -0.378725   \n",
       "2897   7.712275   2.122422  12.012534 -12.924060  ... -0.317056  2.696913   \n",
       "6361 -11.866992  -9.229942  12.799775  -5.462458  ...  0.365749 -0.317567   \n",
       "1654  -8.909064 -14.724182   8.982827 -18.175523  ... -0.938948  0.628654   \n",
       "6111   7.804272   4.631699 -23.114104  -4.831323  ... -0.741045 -0.863689   \n",
       "...         ...        ...        ...        ...  ...       ...       ...   \n",
       "5191  -8.822450 -14.516811   8.961772  -2.359504  ... -1.200718  1.372257   \n",
       "5226   0.452612  16.381729 -22.193395  -6.377645  ...  0.387828  1.204048   \n",
       "5390  14.279390  23.694937   7.549388  12.884744  ...  1.008474  0.574237   \n",
       "860   19.088807   0.907969  12.777942  -0.773037  ...  0.019563  0.030539   \n",
       "7270  -2.821552  -4.350437  -3.697195   3.439736  ... -0.545044  0.257616   \n",
       "\n",
       "       col_492   col_493   col_494   col_495   col_496   col_497   col_498  \\\n",
       "4529 -0.397908 -0.136526 -0.958061 -0.630357 -0.159185 -0.620803 -1.010112   \n",
       "2897 -0.828865  0.567702  0.743005  0.574161 -0.437067  0.293724  1.449087   \n",
       "6361 -1.250173 -0.201837  0.515585  1.730105  0.381383  0.636158 -1.585005   \n",
       "1654  0.792452  0.354764  0.886241 -0.988053 -1.464783  1.549964 -1.125569   \n",
       "6111 -0.039028 -0.456419 -2.576476  0.189589  0.778050 -0.611166  0.340887   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5191  1.042186 -0.230238  1.708837 -0.161753  1.598509  0.781025  0.193675   \n",
       "5226  1.135725  1.584429  1.094804  0.770361  0.520100 -0.526397 -0.241824   \n",
       "5390 -1.141540 -0.756666  0.117051 -1.494126 -1.153968 -0.772677 -0.527955   \n",
       "860  -0.615041  1.538401 -0.869927  1.483099  0.370104  0.253871  0.075206   \n",
       "7270 -0.099697 -0.649687 -1.591489  0.365758  0.204888  1.395029 -0.565957   \n",
       "\n",
       "       col_499  \n",
       "4529 -1.465302  \n",
       "2897 -1.036390  \n",
       "6361 -0.477964  \n",
       "1654 -0.285697  \n",
       "6111 -0.485027  \n",
       "...        ...  \n",
       "5191 -0.694817  \n",
       "5226  1.553872  \n",
       "5390 -0.477227  \n",
       "860  -0.632390  \n",
       "7270  1.040141  \n",
       "\n",
       "[5000 rows x 500 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_features = 500 #20,50,100,250,500\n",
    "n_informative = int(0.90*n_features) #5%,10%,33%,50%,90%\n",
    "n_samples = int(5000/(1-0.33))+1 #7463#5000\n",
    "\n",
    "X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "#X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative,random_state=4,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300/450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6',\n",
       "       'col_16'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapicant \n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "#X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative,random_state=4,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "Inp_db = X.copy(deep=True)\n",
    "Inp_db[\"class\"]=y\n",
    "Inp_db = Inp_db.reset_index()\n",
    "Index_col = \"index\"\n",
    "target_col = \"class\"\n",
    "\n",
    "train_idx,val_idx = train_test_split(Inp_db[\"index\"],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5'], dtype='<U5')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=5000, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, n_repeated = 0,shuffle=False)\n",
    "X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "X[\"class\"]=y\n",
    "X = X.reset_index()\n",
    "\n",
    "\n",
    "explainer_type = shap.TreeExplainer\n",
    "# if classification is False it is a Regression problem\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=False)\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "train_idx,val_idx = train_test_split(X[\"index\"].values,test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = X[X[\"index\"].isin(train_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "X_val = X[X[\"index\"].isin(val_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "Y_train =  X[X[\"index\"].isin(train_idx)][\"class\"]\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "subset = selector.get_features()\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting powershap\n",
      "Automatic mode enabled: Finding the minimal required powershap iterations for significance of 0.01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c386e084f9a047ab9ddc9a81ee536e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 12 iterations; Adding 2  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f860309f83c4dcd92535e186631b55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forcing convergence.\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dda662c9a1045abb1e3f4a3d71fa3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 67  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350510f1ecda4ccbaa312fa65f026b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31934f8ea4454541898806c4904f9412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 45  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c29d46b47f46f0934a37d2e22a4059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 50  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d19623bd2774d17b0950ceadbcf5e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fe879961de4f378291badf1b8c5641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 14 iterations; Adding 4  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1eb4f59e024517a24f252988e98d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 60  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad365743c684955989ae87eb56d28ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f07658ba1c4450185dc05f3f7f9f5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 24  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fbfe0fd2394dd69cca658df85fd463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f967239aaa4a718021142a16101f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 42  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e733d0094db44284a47395966936fc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043c6b21dbb45cdba64442ee9ad6e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 43  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24509a1fdff247a58e08cf43eb7641ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 51  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31df061fe554e74aad67e512637a71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rerunning powershap for convergence. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e064302f776347299603665c21d9d44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No features selected after 10 automatic iterations!\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PowerSHAP(automatic=True, force_convergence=True,\n",
       "          model=<catboost.core.CatBoostClassifier object at 0x000001CA9B147640>,\n",
       "          verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "selector = PowerSHAP(\n",
    "    model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,force_convergence=True,\n",
    ")\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = selector._processed_shaps_df\n",
    "len(t[t.p_value<0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['col_0', 'col_1', 'col_10', 'col_100', 'col_102', 'col_103',\n",
       "       'col_104', 'col_105', 'col_106', 'col_107', 'col_108', 'col_109',\n",
       "       'col_11', 'col_110', 'col_111', 'col_113', 'col_114', 'col_115',\n",
       "       'col_121', 'col_123', 'col_124', 'col_128', 'col_129', 'col_13',\n",
       "       'col_130', 'col_132', 'col_133', 'col_135', 'col_136', 'col_137',\n",
       "       'col_138', 'col_14', 'col_140', 'col_141', 'col_142', 'col_143',\n",
       "       'col_144', 'col_145', 'col_146', 'col_147', 'col_148', 'col_15',\n",
       "       'col_151', 'col_152', 'col_153', 'col_156', 'col_157', 'col_159',\n",
       "       'col_16', 'col_160', 'col_161', 'col_162', 'col_163', 'col_164',\n",
       "       'col_166', 'col_168', 'col_169', 'col_17', 'col_170', 'col_171',\n",
       "       'col_175', 'col_176', 'col_177', 'col_178', 'col_18', 'col_181',\n",
       "       'col_182', 'col_183', 'col_185', 'col_188', 'col_189', 'col_19',\n",
       "       'col_190', 'col_191', 'col_193', 'col_194', 'col_195', 'col_196',\n",
       "       'col_197', 'col_198', 'col_2', 'col_20', 'col_200', 'col_204',\n",
       "       'col_205', 'col_206', 'col_207', 'col_208', 'col_209', 'col_21',\n",
       "       'col_210', 'col_212', 'col_214', 'col_215', 'col_216', 'col_217',\n",
       "       'col_219', 'col_220', 'col_221', 'col_222', 'col_223', 'col_224',\n",
       "       'col_225', 'col_228', 'col_23', 'col_230', 'col_231', 'col_232',\n",
       "       'col_234', 'col_235', 'col_236', 'col_239', 'col_240', 'col_242',\n",
       "       'col_243', 'col_244', 'col_245', 'col_247', 'col_248', 'col_249',\n",
       "       'col_25', 'col_250', 'col_252', 'col_256', 'col_257', 'col_258',\n",
       "       'col_259', 'col_26', 'col_261', 'col_262', 'col_263', 'col_265',\n",
       "       'col_266', 'col_27', 'col_271', 'col_272', 'col_273', 'col_274',\n",
       "       'col_275', 'col_277', 'col_278', 'col_279', 'col_28', 'col_282',\n",
       "       'col_285', 'col_286', 'col_287', 'col_288', 'col_29', 'col_290',\n",
       "       'col_293', 'col_294', 'col_296', 'col_297', 'col_298', 'col_299',\n",
       "       'col_3', 'col_30', 'col_301', 'col_302', 'col_303', 'col_305',\n",
       "       'col_306', 'col_307', 'col_308', 'col_31', 'col_310', 'col_311',\n",
       "       'col_313', 'col_314', 'col_315', 'col_316', 'col_317', 'col_318',\n",
       "       'col_32', 'col_320', 'col_321', 'col_323', 'col_325', 'col_327',\n",
       "       'col_329', 'col_330', 'col_333', 'col_334', 'col_335', 'col_337',\n",
       "       'col_338', 'col_339', 'col_34', 'col_340', 'col_341', 'col_342',\n",
       "       'col_343', 'col_345', 'col_347', 'col_348', 'col_35', 'col_350',\n",
       "       'col_351', 'col_352', 'col_353', 'col_354', 'col_355', 'col_356',\n",
       "       'col_357', 'col_358', 'col_362', 'col_363', 'col_364', 'col_365',\n",
       "       'col_366', 'col_369', 'col_370', 'col_371', 'col_372', 'col_375',\n",
       "       'col_376', 'col_377', 'col_378', 'col_379', 'col_380', 'col_383',\n",
       "       'col_384', 'col_387', 'col_388', 'col_39', 'col_390', 'col_392',\n",
       "       'col_393', 'col_395', 'col_396', 'col_397', 'col_399', 'col_4',\n",
       "       'col_40', 'col_400', 'col_401', 'col_402', 'col_403', 'col_404',\n",
       "       'col_405', 'col_407', 'col_408', 'col_409', 'col_41', 'col_410',\n",
       "       'col_412', 'col_413', 'col_417', 'col_419', 'col_42', 'col_421',\n",
       "       'col_423', 'col_426', 'col_427', 'col_428', 'col_429', 'col_43',\n",
       "       'col_430', 'col_431', 'col_432', 'col_434', 'col_435', 'col_437',\n",
       "       'col_438', 'col_44', 'col_440', 'col_441', 'col_442', 'col_443',\n",
       "       'col_444', 'col_446', 'col_447', 'col_448', 'col_449', 'col_45',\n",
       "       'col_46', 'col_465', 'col_47', 'col_491', 'col_5', 'col_50',\n",
       "       'col_51', 'col_52', 'col_53', 'col_54', 'col_55', 'col_56',\n",
       "       'col_57', 'col_59', 'col_6', 'col_60', 'col_62', 'col_63',\n",
       "       'col_65', 'col_66', 'col_67', 'col_68', 'col_69', 'col_70',\n",
       "       'col_71', 'col_73', 'col_74', 'col_75', 'col_77', 'col_78',\n",
       "       'col_79', 'col_8', 'col_80', 'col_81', 'col_82', 'col_83',\n",
       "       'col_84', 'col_85', 'col_86', 'col_87', 'col_88', 'col_89',\n",
       "       'col_9', 'col_90', 'col_91', 'col_93', 'col_94', 'col_95',\n",
       "       'col_96', 'col_97', 'col_98', 'col_99'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(t[t.p_value<0.01].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One by one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "selector = PowerSHAP(\n",
    "    model = LogisticRegressionCV(max_iter=1000),# CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=200,automatic=False, limit_automatic=10,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_auto = PowerSHAP(\n",
    "    model = LogisticRegressionCV(max_iter=1000),#CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    #model = CatBoostRegressor(verbose=0, n_estimators=0,use_best_model=True),#GradientBoostingClassifier(),#CatBoostClassifier(verbose=0, n_estimators=250),\n",
    "    power_iterations=200,automatic=True, limit_automatic=10,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.fit(X_train, y_train)\n",
    "selector_auto.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = selector_auto._processed_shaps_df\n",
    "t_inf = np.sum(np.isin(X_train.columns.values[:int(0.9*n_features)],t[t.p_value<0.01].index.values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import powerSHAP_statistical_analysis\n",
    "\n",
    "shaps_df = selector._shaps_df\n",
    "found_features = []\n",
    "found_informative_features = []\n",
    "found_noise_features = []\n",
    "\n",
    "processed_shaps_df = powerSHAP_statistical_analysis(\n",
    "                        shaps_df[:37],\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        include_all=True,\n",
    ")\n",
    "processed_shaps_df = processed_shaps_df[processed_shaps_df.index.values!=\"random_uniform_feature\"]\n",
    "\n",
    "for i in (range(1,100)):\n",
    "    \n",
    "    found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<i/100]))\n",
    "    found_informative_features.append(np.sum(np.isin(X_train.columns.values[:int(0.33*n_features)],X_train.columns.values[processed_shaps_df[processed_shaps_df.p_value<i/100].index.values.astype(np.int32)])) )\n",
    "    found_noise_features.append(np.sum(1-np.isin(X_train.columns.values[processed_shaps_df[processed_shaps_df.p_value<i/100].index.values.astype(np.int32)],X_train.columns.values[:int(0.33*n_features)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = powerSHAP_statistical_analysis(\n",
    "                        shaps_df[:27],\n",
    "                        0.01,\n",
    "                        0.99,\n",
    "                        include_all=False,\n",
    "    )\n",
    "t_inf = np.sum(np.isin(X_train.columns.values[:int(0.33*n_features)],X_train.columns.values[t[t.p_value<0.01].index.values.astype(np.int32)])) \n",
    "t_noise = np.sum(1-np.isin(X_train.columns.values[t[t.p_value<0.01].index.values.astype(np.int32)],X_train.columns.values[:int(0.33*n_features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs=plt.subplots(1,1,figsize=(6,5))\n",
    "axs.plot(range(1,100),np.array(found_features),label=\"All outputted features\",alpha=0.5)\n",
    "axs.plot(range(1,100),np.array(found_informative_features),label=\"Informative features found\")\n",
    "axs.plot(range(1,100),np.repeat([83],100-1),label=\"True Informative features\",color='grey',alpha=0.5)\n",
    "axs.plot([26.99,27],[0,140],color=\"red\",label=\"Automatic mode\",alpha=0.5)\n",
    "axs.plot(range(1,100),np.array(found_noise_features),label=\"Noise features\")\n",
    "axs.set_ylabel(\"Found features\")\n",
    "axs.set_xlabel(\"Powershap Iterations\")\n",
    "axs.legend()\n",
    "#axs.set_ylim([0,140])\n",
    "#axs[2].plot(range(2,200),np.array(found_noise_features))\n",
    "#axs[2].scatter(27,t_noise,color=\"red\")\n",
    "#axs[2].set_ylim([0,110])\n",
    "#axs[2].set_ylabel(\"Found noise features\")\n",
    "#axs[2].set_xlabel(\"Iterations\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.jpg\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.pdf\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.svg\", bbox_inches=\"tight\")\n",
    "#f.savefig(\"logisticregressioncv_iterations_analysis.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"selector\",\n",
    "            PowerSHAP(\n",
    "                CatBoostClassifier(n_estimators=250,verbose=False,use_best_model=True), automatic=True, limit_automatic=100,#power_alpha=0.001,power_req_iterations=0.999,\n",
    "                #CatBoostRegressor(n_estimators=250,verbose=False), automatic=True, limit_automatic=100,\n",
    "            ),\n",
    "        ),\n",
    "        (\"catboost\", KNeighborsClassifier()),#(n_estimators=250,verbose=False)),\n",
    "        #(\"catboost\", CatBoostRegressor(n_estimators=250,verbose=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "\n",
    "\n",
    "print(\"Baseline\", accuracy_score(KNeighborsClassifier().fit(X_train, y_train).predict(X_test), y_test))\n",
    "#print(\"Baseline\", r2_score(LinearRegression.fit(X_train, y_train).predict(X_test), y_test))\n",
    "\n",
    "\n",
    "print(\"PowerShap feature selection:\", accuracy_score(pipe.predict(X_test), y_test))\n",
    "#print(\"PowerShap feature selection:\", r2_score(pipe.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline\", accuracy_score(CatBoostClassifier(verbose=False,n_estimators=250).fit(X_train, y_train).predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_shaps_df = pipe[0]._processed_shaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_shaps_df[processed_shaps_df.p_value<0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dict_print(output_dict):\n",
    "    for key in output_dict:\n",
    "        #samples\n",
    "        print(key+\": \")\n",
    "        for key_inner in output_dict[key]:\n",
    "            #features\n",
    "            print(key_inner+\": \")\n",
    "            \n",
    "            #per informative\n",
    "            time_str = \"\"\n",
    "            \n",
    "            i = 0\n",
    "            for avg_time in output_dict[key][key_inner][\"Average time\"]:\n",
    "                \n",
    "                i = i+1\n",
    "                \n",
    "                if i ==len(output_dict[key][key_inner][\"Average time\"]):\n",
    "                    time_str = time_str + str(avg_time)+\"s | \"\n",
    "                else:\n",
    "                    time_str = time_str + str(avg_time)+\"s - \"\n",
    "                  \n",
    "            key_strs = \"\"\n",
    "            for key_inner_inner in output_dict[key][key_inner]: \n",
    "                if key_inner_inner != 'Average time':\n",
    "                    key_strs = key_strs + str(output_dict[key][key_inner][key_inner_inner][\"found_informative_features\"]) + \" (\"+str(output_dict[key][key_inner][key_inner_inner][\"outputted_noise_features\"])+\") / \"+str(output_dict[key][key_inner][key_inner_inner][\"informative_features\"]) + \" | \"\n",
    "                \n",
    "            print(time_str+key_strs)\n",
    "                 \n",
    "\n",
    "def output_dict_to_df(output_dict):\n",
    "    df = pd.DataFrame(columns=[\"n_samples\",\"total_features\",\"informative_features\",\"time\",\"seed\",\"found_informative_features\",\"outputted_noise_features\"])\n",
    "    for samples in output_dict.keys():\n",
    "        for features in output_dict[samples].keys():\n",
    "            i = 0\n",
    "            for informative in output_dict[samples][features].keys():\n",
    "                if informative != 'Average time':\n",
    "                    temp = pd.DataFrame.from_dict(output_dict[samples][features][informative])\n",
    "                    temp[\"n_samples\"]=int(samples)\n",
    "                    temp[\"total_features\"]=int(features)\n",
    "                    temp = temp.reset_index()\n",
    "                    temp = temp.rename(columns={\"index\":\"seed\"})\n",
    "                    temp[\"time\"]=output_dict[samples][features]['Average time'][i]\n",
    "                    i = i+1\n",
    "                    df = df.append(temp) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 50 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "import pprint\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 50#250\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_50_Classification_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 250 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        if hypercube:\n",
    "            output_dict_to_df(output_dict).to_csv(\"estimators_250_Classification_output_df.csv\",index=False)\n",
    "        else:\n",
    "            output_dict_to_df(output_dict).to_csv(\"estimators_250_Classification_output_df_polytone.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [50,100]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            for n_redundant in [10,25,33,50]:\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "                \n",
    "                n_redundant_samples = int((n_features-int(n_informative/100*n_features))*(n_redundant/100))\n",
    "                \n",
    "                print(\"Amount of samples = \"+str(n_samples))\n",
    "                print(\"Total used features = \"+str(n_features))\n",
    "                print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "                print(\"Redundant features: \"+str(n_redundant_samples)+\" (\"+str(n_redundant)+\"%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                found_features = []\n",
    "                found_idx_features = []\n",
    "                times = []\n",
    "                for random_seed in [0,1,2,3,4]:\n",
    "                    print(\"Seed \"+str(random_seed))\n",
    "\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=n_redundant_samples, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                    X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    if regression_bool:\n",
    "                        selector = PowerSHAP(\n",
    "                            model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                            automatic=True\n",
    "                        )\n",
    "                    else:\n",
    "                        selector = PowerSHAP(\n",
    "                            model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                            automatic=True\n",
    "                        )\n",
    "                    selector.fit(X, y)\n",
    "\n",
    "                    times.append(time.time() - start_time)\n",
    "\n",
    "                    processed_shaps_df = selector._processed_shaps_df\n",
    "                    print(50*\"-\")\n",
    "\n",
    "                    found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                    found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "\n",
    "                found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "                found_redundant = [np.sum(np.isin(X.columns.values[int(n_informative/100*n_features):int(n_informative/100*n_features)+n_redundant_samples],f_list)) for f_list in found_idx_features]\n",
    "                found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)+n_redundant_samples])) for f_list in found_idx_features]\n",
    "                print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "                print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found redundant: \"+str(found_redundant) + \"/\"+str(n_redundant_samples))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "                print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "\n",
    "                average_times.append(np.round(times,2))\n",
    "\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "\n",
    "                print(100*\"=\")\n",
    "\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 250\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [50]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            for n_redundant in [10,25,33,50]:\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "                \n",
    "                n_redundant_samples = int((n_features-int(n_informative/100*n_features))*(n_redundant/100))\n",
    "                \n",
    "                print(\"Amount of samples = \"+str(n_samples))\n",
    "                print(\"Total used features = \"+str(n_features))\n",
    "                print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "                print(\"Redundant features: \"+str(n_redundant_samples)+\" (\"+str(n_redundant)+\"%)\")\n",
    "                print(\"\")\n",
    "                \n",
    "                found_features = []\n",
    "                found_idx_features = []\n",
    "                times = []\n",
    "                for random_seed in [0,1,2,3,4]:\n",
    "                    print(\"Seed \"+str(random_seed))\n",
    "\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, hypercube=hypercube, n_informative=int(n_informative/100*n_features), n_redundant=n_redundant_samples, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                    X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    # if classification is False it is a Regression problem\n",
    "                    model = CatBoostClassifier(verbose=0, n_estimators=250)\n",
    "                    selector = BorutaShap(model=model,importance_measure='shap',classification=True)\n",
    "                    selector.fit(X, y,verbose=False)\n",
    "\n",
    "                    times.append(time.time() - start_time)\n",
    "\n",
    "                    subset = selector.Subset()\n",
    "                    print(50*\"-\")\n",
    "\n",
    "                    found_features.append(len(subset.columns))\n",
    "                    found_idx_features.append(subset.columns)\n",
    "\n",
    "                found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "                found_redundant = [np.sum(np.isin(X.columns.values[int(n_informative/100*n_features):int(n_informative/100*n_features)+n_redundant_samples],f_list)) for f_list in found_idx_features]\n",
    "                found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)+n_redundant_samples])) for f_list in found_idx_features]\n",
    "                print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "                print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found redundant: \"+str(found_redundant) + \"/\"+str(n_redundant_samples))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "                print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "                print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "\n",
    "                average_times.append(np.round(times,2))\n",
    "\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "                output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "\n",
    "                print(100*\"=\")\n",
    "\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 500 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "regression_bool=False\n",
    "estimators = 500\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        output_dict_to_df(output_dict).to_csv(\"estimators_500_Classification_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 50 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 50\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_50_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 250 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 250\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_250_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators = 500 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=True\n",
    "estimators = 500\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                if regression_bool:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostRegressor(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                else:\n",
    "                    selector = PowerSHAP(\n",
    "                        model = CatBoostClassifier(verbose=0, n_estimators=estimators,use_best_model=True),\n",
    "                        automatic=True\n",
    "                    )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "        \n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"estimators_500_Regression_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV,LinearRegression\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "hypercube = False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,hypercube=hypercube,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                selector = PowerSHAP(\n",
    "                    model = LogisticRegressionCV(max_iter=1000),\n",
    "                    automatic=True\n",
    "                )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "        if hypercube:\n",
    "            output_dict_to_df(output_dict).to_csv(\"logisticregressioncv_output_df.csv\",index=False)\n",
    "        else:\n",
    "            output_dict_to_df(output_dict).to_csv(\"logisticregressioncv_output_df_polytone.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                selector = PowerSHAP(\n",
    "                    model = RandomForestClassifier(),\n",
    "                    automatic=True\n",
    "                )\n",
    "                selector.fit(X, y)\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                \n",
    "                processed_shaps_df = selector._processed_shaps_df\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(processed_shaps_df[processed_shaps_df.p_value<0.01]))\n",
    "                found_idx_features.append(processed_shaps_df[processed_shaps_df.p_value<0.01].index.values)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "        print(100*\"=\")\n",
    "    \n",
    "output_dict_to_df(output_dict).to_csv(\"randomforest_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopped simulation at 5000 samples, 20000 samples takes on average 300s per model! so at least 3000s per seed. \n",
    "output_dict_to_df(output_dict).to_csv(\"randomforest_output_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BorutaShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # if classification is False it is a Regression problem\n",
    "                model = CatBoostClassifier(verbose=0, n_estimators=250)\n",
    "                selector = BorutaShap(model=model,importance_measure='shap',classification=True)\n",
    "\n",
    "                selector.fit(X=X, y=y, verbose=False)\n",
    "                subset = selector.Subset()\n",
    "                \n",
    "                times.append(time.time() - start_time)\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(subset.columns))\n",
    "                found_idx_features.append(subset.columns)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "            \n",
    "        output_dict_to_df(output_dict).to_csv(\"250_est_Catboost_borutashap_output_df.csv\",index=False)\n",
    "        \n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of samples = 5000\n",
      "Total used features = 20\n",
      "Informative features: 2 (10%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:09<00:00,  1.30s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:12<00:00,  1.33s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.38s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:07<00:00,  1.28s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 131.1 seconds\n",
      "Found features: [9, 6, 8, 5, 6]\n",
      "Found 1.0 of 2 informative features\n",
      "5.8 of 6.8 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 20\n",
      "Informative features: 6 (33%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 118.89 seconds\n",
      "Found features: [6, 6, 6, 6, 8]\n",
      "Found 5.0 of 6 informative features\n",
      "1.4 of 6.4 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 20\n",
      "Informative features: 10 (50%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 118.71 seconds\n",
      "Found features: [10, 10, 11, 10, 10]\n",
      "Found 9.0 of 10 informative features\n",
      "1.2 of 10.2 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 20\n",
      "Informative features: 18 (90%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:04<00:00,  1.24s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 120.72 seconds\n",
      "Found features: [18, 18, 18, 18, 18]\n",
      "Found 17.0 of 18 informative features\n",
      "1.0 of 18.0 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "5000: \n",
      "20: \n",
      "[129.96 132.72 138.2  126.62 128.01]s - [118.57 118.89 119.58 118.4  119.01]s - [118.78 118.95 118.63 118.46 118.72]s - [119.67 121.64 124.11 118.92 119.25]s | [1, 1, 1, 1, 1] ([8, 5, 7, 4, 5]) / 2 | [5, 5, 5, 5, 5] ([1, 1, 1, 1, 3]) / 6 | [9, 9, 9, 9, 9] ([1, 1, 2, 1, 1]) / 10 | [17, 17, 17, 17, 17] ([1, 1, 1, 1, 1]) / 18 | \n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 100\n",
      "Informative features: 10 (10%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:22<00:00,  3.23s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:41<00:00,  3.41s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:29<00:00,  3.29s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:52<00:00,  3.52s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 332.93 seconds\n",
      "Found features: [14, 10, 12, 11, 10]\n",
      "Found 9.0 of 10 informative features\n",
      "2.4 of 11.4 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 100\n",
      "Informative features: 33 (33%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:56<00:00,  3.56s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:12<00:00,  3.72s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:05<00:00,  3.66s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:04<00:00,  3.65s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:02<00:00,  3.62s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 364.36 seconds\n",
      "Found features: [32, 33, 32, 32, 33]\n",
      "Found 31.4 of 33 informative features\n",
      "1.0 of 32.4 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 100\n",
      "Informative features: 50 (50%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:18<00:00,  3.78s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:15<00:00,  3.76s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:07<00:00,  3.67s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:06<00:00,  3.67s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:15<00:00,  3.76s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 372.76 seconds\n",
      "Found features: [42, 40, 45, 45, 43]\n",
      "Found 42.0 of 50 informative features\n",
      "1.0 of 43.0 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 100\n",
      "Informative features: 90 (90%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:13<00:00,  3.73s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:15<00:00,  3.75s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:12<00:00,  3.72s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [06:14<00:00,  3.74s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [05:41<00:00,  3.41s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 367.28 seconds\n",
      "Found features: [57, 61, 46, 55, 62]\n",
      "Found 55.2 of 90 informative features\n",
      "1.0 of 56.2 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "5000: \n",
      "20: \n",
      "[129.96 132.72 138.2  126.62 128.01]s - [118.57 118.89 119.58 118.4  119.01]s - [118.78 118.95 118.63 118.46 118.72]s - [119.67 121.64 124.11 118.92 119.25]s | [1, 1, 1, 1, 1] ([8, 5, 7, 4, 5]) / 2 | [5, 5, 5, 5, 5] ([1, 1, 1, 1, 3]) / 6 | [9, 9, 9, 9, 9] ([1, 1, 2, 1, 1]) / 10 | [17, 17, 17, 17, 17] ([1, 1, 1, 1, 1]) / 18 | \n",
      "100: \n",
      "[322.99 318.8  341.21 329.51 352.14]s - [356.4  372.1  365.94 364.89 362.47]s - [378.24 375.55 367.15 366.87 376.  ]s - [373.45 375.08 372.11 374.3  341.48]s | [9, 9, 9, 9, 9] ([5, 1, 3, 2, 1]) / 10 | [31, 32, 31, 31, 32] ([1, 1, 1, 1, 1]) / 33 | [41, 39, 43, 45, 42] ([1, 1, 2, 0, 1]) / 50 | [56, 60, 45, 54, 61] ([1, 1, 1, 1, 1]) / 90 | \n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 250\n",
      "Informative features: 25 (10%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [11:54<00:00,  7.15s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [11:35<00:00,  6.95s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [11:54<00:00,  7.15s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [12:36<00:00,  7.57s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [13:32<00:00,  8.12s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 738.67 seconds\n",
      "Found features: [28, 25, 27, 26, 25]\n",
      "Found 24.0 of 25 informative features\n",
      "2.2 of 26.2 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 250\n",
      "Informative features: 82 (33%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [31:02<00:00, 18.63s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:59<00:00,  6.59s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:55<00:00,  6.56s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [11:09<00:00,  6.69s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:54<00:00,  6.55s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 900.46 seconds\n",
      "Found features: [62, 63, 62, 68, 64]\n",
      "Found 62.8 of 82 informative features\n",
      "1.0 of 63.8 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 250\n",
      "Informative features: 125 (50%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:56<00:00,  6.56s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [11:01<00:00,  6.61s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:53<00:00,  6.54s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:51<00:00,  6.51s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:48<00:00,  6.48s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 654.19 seconds\n",
      "Found features: [75, 66, 74, 79, 76]\n",
      "Found 73.2 of 125 informative features\n",
      "0.8 of 74.0 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 250\n",
      "Informative features: 225 (90%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:47<00:00,  6.48s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:48<00:00,  6.48s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:47<00:00,  6.48s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:47<00:00,  6.47s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:44<00:00,  6.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 647.16 seconds\n",
      "Found features: [86, 83, 77, 70, 81]\n",
      "Found 78.6 of 225 informative features\n",
      "0.8 of 79.4 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "5000: \n",
      "20: \n",
      "[129.96 132.72 138.2  126.62 128.01]s - [118.57 118.89 119.58 118.4  119.01]s - [118.78 118.95 118.63 118.46 118.72]s - [119.67 121.64 124.11 118.92 119.25]s | [1, 1, 1, 1, 1] ([8, 5, 7, 4, 5]) / 2 | [5, 5, 5, 5, 5] ([1, 1, 1, 1, 3]) / 6 | [9, 9, 9, 9, 9] ([1, 1, 2, 1, 1]) / 10 | [17, 17, 17, 17, 17] ([1, 1, 1, 1, 1]) / 18 | \n",
      "100: \n",
      "[322.99 318.8  341.21 329.51 352.14]s - [356.4  372.1  365.94 364.89 362.47]s - [378.24 375.55 367.15 366.87 376.  ]s - [373.45 375.08 372.11 374.3  341.48]s | [9, 9, 9, 9, 9] ([5, 1, 3, 2, 1]) / 10 | [31, 32, 31, 31, 32] ([1, 1, 1, 1, 1]) / 33 | [41, 39, 43, 45, 42] ([1, 1, 2, 0, 1]) / 50 | [56, 60, 45, 54, 61] ([1, 1, 1, 1, 1]) / 90 | \n",
      "250: \n",
      "[714.77 695.09 714.55 756.88 812.07]s - [1863.01  659.39  655.92  669.34  654.66]s - [656.42 661.05 653.62 651.49 648.38]s - [647.94 648.45 647.94 647.44 644.04]s | [24, 24, 24, 24, 24] ([4, 1, 3, 2, 1]) / 25 | [61, 63, 61, 67, 62] ([1, 0, 1, 1, 2]) / 82 | [74, 66, 73, 78, 75] ([1, 0, 1, 1, 1]) / 125 | [85, 82, 75, 70, 81] ([1, 1, 2, 0, 0]) / 225 | \n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 500\n",
      "Informative features: 50 (10%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [23:39<00:00, 14.20s/it]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [25:21<00:00, 15.21s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [22:20<00:00, 13.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [22:28<00:00, 13.48s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [21:21<00:00, 12.81s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 1382.18 seconds\n",
      "Found features: [50, 49, 48, 49, 52]\n",
      "Found 47.6 of 50 informative features\n",
      "2.0 of 49.6 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 500\n",
      "Informative features: 165 (33%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [22:00<00:00, 13.20s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [22:44<00:00, 13.65s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:33<00:00, 12.34s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:31<00:00, 12.31s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:31<00:00, 12.31s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 1276.27 seconds\n",
      "Found features: [92, 88, 92, 100, 88]\n",
      "Found 91.0 of 165 informative features\n",
      "1.0 of 92.0 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 500\n",
      "Informative features: 250 (50%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:27<00:00, 12.27s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:26<00:00, 12.26s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:30<00:00, 12.30s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:39<00:00, 12.40s/it]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:28<00:00, 12.29s/it]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 1230.61 seconds\n",
      "Found features: [104, 101, 94, 88, 96]\n",
      "Found 96.0 of 250 informative features\n",
      "0.6 of 96.6 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "Amount of samples = 5000\n",
      "Total used features = 500\n",
      "Informative features: 450 (90%)\n",
      "\n",
      "Seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:25<00:00, 12.26s/it]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:26<00:00, 12.26s/it]\n",
      "Computing true SHAP values:   0%|                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:30<00:00, 12.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:25<00:00, 12.26s/it]\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Seed 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [20:25<00:00, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Average time: 1226.84 seconds\n",
      "Found features: [93, 103, 93, 98, 103]\n",
      "Found 97.8 of 450 informative features\n",
      "0.2 of 98.0 outputted powershap features are noise features\n",
      "====================================================================================================\n",
      "5000: \n",
      "20: \n",
      "[129.96 132.72 138.2  126.62 128.01]s - [118.57 118.89 119.58 118.4  119.01]s - [118.78 118.95 118.63 118.46 118.72]s - [119.67 121.64 124.11 118.92 119.25]s | [1, 1, 1, 1, 1] ([8, 5, 7, 4, 5]) / 2 | [5, 5, 5, 5, 5] ([1, 1, 1, 1, 3]) / 6 | [9, 9, 9, 9, 9] ([1, 1, 2, 1, 1]) / 10 | [17, 17, 17, 17, 17] ([1, 1, 1, 1, 1]) / 18 | \n",
      "100: \n",
      "[322.99 318.8  341.21 329.51 352.14]s - [356.4  372.1  365.94 364.89 362.47]s - [378.24 375.55 367.15 366.87 376.  ]s - [373.45 375.08 372.11 374.3  341.48]s | [9, 9, 9, 9, 9] ([5, 1, 3, 2, 1]) / 10 | [31, 32, 31, 31, 32] ([1, 1, 1, 1, 1]) / 33 | [41, 39, 43, 45, 42] ([1, 1, 2, 0, 1]) / 50 | [56, 60, 45, 54, 61] ([1, 1, 1, 1, 1]) / 90 | \n",
      "250: \n",
      "[714.77 695.09 714.55 756.88 812.07]s - [1863.01  659.39  655.92  669.34  654.66]s - [656.42 661.05 653.62 651.49 648.38]s - [647.94 648.45 647.94 647.44 644.04]s | [24, 24, 24, 24, 24] ([4, 1, 3, 2, 1]) / 25 | [61, 63, 61, 67, 62] ([1, 0, 1, 1, 2]) / 82 | [74, 66, 73, 78, 75] ([1, 0, 1, 1, 1]) / 125 | [85, 82, 75, 70, 81] ([1, 1, 2, 0, 0]) / 225 | \n",
      "500: \n",
      "[1419.87 1521.14 1340.27 1348.49 1281.12]s - [1320.06 1364.73 1233.99 1231.23 1231.33]s - [1227.46 1226.36 1230.47 1239.89 1228.85]s - [1225.96 1226.12 1230.79 1225.64 1225.67]s | [48, 48, 46, 48, 48] ([2, 1, 2, 1, 4]) / 50 | [92, 88, 91, 97, 87] ([0, 0, 1, 3, 1]) / 165 | [104, 100, 94, 88, 94] ([0, 1, 0, 0, 2]) / 250 | [92, 103, 93, 98, 103] ([1, 0, 0, 0, 0]) / 450 | \n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification,make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "#sys.path.append(\"../powershap\")\n",
    "\n",
    "from powershap import PowerSHAP\n",
    "import shapicant\n",
    "\n",
    "#n_features = 20 #20,50,100,250,500\n",
    "#n_informative = int(0.10*n_features) #10%,33%,50%,90%\n",
    "#n_samples = 1000#5000\n",
    "\n",
    "regression_bool=False\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for n_samples in [5000]:#1000,5000,20000]:\n",
    "    output_dict[str(n_samples)]={}\n",
    "    for n_features in [20,100,250,500]:#[20,50,100,250,500]:\n",
    "        output_dict[str(n_samples)][str(n_features)]={}\n",
    "        \n",
    "        average_times = []\n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        for n_informative in [10,33,50,90]:#[int(0.10*n_features),int(0.33*n_features),int(0.50*n_features),int(0.90*n_features)]:\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"]={}\n",
    "            print(\"Amount of samples = \"+str(n_samples))\n",
    "            print(\"Total used features = \"+str(n_features))\n",
    "            print(\"Informative features: \"+str(int(n_informative/100*n_features))+\" (\"+str(n_informative)+\"%)\")\n",
    "            print(\"\")\n",
    "            \n",
    "            found_features = []\n",
    "            found_idx_features = []\n",
    "            times = []\n",
    "            for random_seed in [0,1,2,3,4]:\n",
    "                print(\"Seed \"+str(random_seed))\n",
    "                \n",
    "                if regression_bool:\n",
    "                    X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_informative/100*n_features),random_state=random_seed,shuffle=False)\n",
    "                else:\n",
    "                    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=int(n_informative/100*n_features), n_redundant=0, n_repeated = 0,shuffle=False,random_state=random_seed)\n",
    "                X = pd.DataFrame(data=X, columns=[f\"col_{i}\" for i in range(n_features)])\n",
    "                X[\"class\"]=y\n",
    "                X = X.reset_index()\n",
    "\n",
    "                \n",
    "                explainer_type = shap.TreeExplainer\n",
    "                # if classification is False it is a Regression problem\n",
    "                model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=False)\n",
    "                selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "                train_idx,val_idx = train_test_split(X[\"index\"].values,test_size=0.2,random_state = 0)\n",
    "\n",
    "                X_train = X[X[\"index\"].isin(train_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "                X_val = X[X[\"index\"].isin(val_idx)].copy(deep=True)[list(X.columns.values[1:-1])]\n",
    "                Y_train =  X[X[\"index\"].isin(train_idx)][\"class\"]\n",
    "\n",
    "                # Run the feature selection\n",
    "                # If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "                # We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                selector.fit(X_train, Y_train, X_validation=X_val)\n",
    "\n",
    "                subset = selector.get_features()\n",
    "                p_values = selector.p_values_\n",
    "\n",
    "                np.array(subset)\n",
    "\n",
    "                times.append(time.time() - start_time)\n",
    "                print(50*\"-\")\n",
    "                \n",
    "                found_features.append(len(subset))\n",
    "                found_idx_features.append(subset)\n",
    "                \n",
    "            found_informative_features = [np.sum(np.isin(X.columns.values[:int(n_informative/100*n_features)],f_list)) for f_list in found_idx_features]\n",
    "            found_noise_features = [np.sum(1-np.isin(f_list,X.columns.values[:int(n_informative/100*n_features)])) for f_list in found_idx_features]\n",
    "            print(\"Average time: \"+str(np.round(np.mean(times),2))+\" seconds\")\n",
    "            print(\"Found features: \"+str(found_features))#len(processed_shaps_df[processed_shaps_df.p_value<0.01])))\n",
    "            print(\"Found \"+str(np.mean(found_informative_features))+\" of \"+str(int(n_informative/100*n_features))+\" informative features\")\n",
    "            print(str(np.mean(found_noise_features))+\" of \"+str(np.mean(found_features))+\" outputted powershap features are noise features\")\n",
    "            \n",
    "            average_times.append(np.round(times,2))\n",
    "            \n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"informative_features\"]=int(n_informative/100*n_features)\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"found_informative_features\"]=found_informative_features\n",
    "            output_dict[str(n_samples)][str(n_features)][str(n_informative)+\"%\"][\"outputted_noise_features\"]=found_noise_features\n",
    "            \n",
    "            print(100*\"=\")\n",
    "            \n",
    "        output_dict[str(n_samples)][str(n_features)][\"Average time\"]=average_times\n",
    "        benchmark_dict_print(output_dict)\n",
    "            \n",
    "        output_dict_to_df(output_dict).to_csv(\"shapicant_output_df.csv\",index=False)\n",
    "        \n",
    "        print(100*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4454adb76e1590f06b0cd848f1b87f82b41871aa5648994c4db0c9fbdc1ef435"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
