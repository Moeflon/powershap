{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randrange\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt, erf, exp\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report,auc,r2_score,matthews_corrcoef,roc_auc_score\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from catboost.utils import get_roc_curve\n",
    "import shap\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from sklearn import preprocessing\n",
    "from catboost import Pool, cv\n",
    "from scipy import stats\n",
    "import copy\n",
    "from sklearn import feature_selection as fs\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "from numpy.random import RandomState\n",
    "from sklearn import metrics as me\n",
    "from BorutaShap import BorutaShap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_selection import chi2,f_classif,f_regression,r_regression\n",
    "\n",
    "from PowerSHAP.powershap  import PowerSHAP\n",
    "import shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_calc_print(Y,Y_pred,print_bool):\n",
    "    if len(Y_pred) > 1:\n",
    "        R2_total = me.r2_score(Y,Y_pred)\n",
    "    else:\n",
    "        R2_total = -1\n",
    "    RMSE_total = sqrt(me.mean_squared_error(Y,Y_pred))\n",
    "    MAE_total = me.mean_absolute_error(Y,Y_pred)\n",
    "    ME_total = 0\n",
    "    \n",
    "    diff = np.subtract(Y_pred,Y)\n",
    "    for i in range(0,len(diff)):\n",
    "        ME_total+=diff[i]\n",
    "        \n",
    "    ME_total = ME_total/len(diff)\n",
    "    \n",
    "    if print_bool:\n",
    "        print(tabulate([[RMSE_total, MAE_total,R2_total,ME_total]], [\"RMSE\",\"MAE\",\"R²\",\"ME\"], tablefmt=\"grid\"))\n",
    "        #print(f\"RMSE of total: {RMSE_total:.4f}\")\n",
    "        #print(f\"MAE of total: {MAE_total:.4f}\")\n",
    "        #print(f\"R² of total: {R2_total:.4f}\")\n",
    "        #print(f\"ME of total: {ME_total:.4f}\")\n",
    "        #print(f\"MAPE of total: {MAPE_total:.4f}%\")\n",
    "        #print(f\"MdAPE of total: {MdAPE_total:.4f}%\")\n",
    "        #print(f\"MdPE of total: {MdPE_total:.4f}%\")\n",
    "        #print(f\"Std of results: {std_arr:.4f}\")\n",
    "        #print(\"\\n\")\n",
    "    else:\n",
    "        return {\"R2\":R2_total,\"RMSE\":RMSE_total,\"MAE\":MAE_total,\"ME\":ME_total}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,classification_report,auc,r2_score,matthews_corrcoef\n",
    "\n",
    "## This cross_validation is different from the standard cross validation, because the a priori dataset is only tested on actual a priori samples\n",
    "# Inp_db:pandas_DataFrame = input database (contains labels and features), should contain heroi2c numbers as IDs, previous concentrations\n",
    "# Features:List = features to use\n",
    "# Folds:int = amount of folds\n",
    "# RS:int = Random state\n",
    "# Output:dict = dictionary with the results of each fold\n",
    "def benchmark_classification_cross_validation(Model,Inp_db,index_col,folds,RS,features,target_col,disable_tqdm_output=False):\n",
    "    kf = KFold(n_splits=folds,shuffle=True,random_state=RS)\n",
    "\n",
    "    scores_cv_train = {\"AUC\":np.array([]),\n",
    "                 \"MCC\":np.array([]),      \n",
    "                 \"ACCURACY\":np.array([]),\n",
    "                 \"RECALL\":np.array([]),\n",
    "                 \"F1\":np.array([]),\n",
    "                 \"PRECISION\":np.array([])}\n",
    "    \n",
    "    scores_cv_test = {\"AUC\":np.array([]),\n",
    "                 \"MCC\":np.array([]),      \n",
    "                 \"ACCURACY\":np.array([]),\n",
    "                 \"RECALL\":np.array([]),\n",
    "                 \"F1\":np.array([]),\n",
    "                 \"PRECISION\":np.array([])}\n",
    "    \n",
    "    for CV_train_idx, CV_test_idx in tqdm(kf.split(Inp_db[index_col].unique()),disable=disable_tqdm_output):  \n",
    "        ## Split per patient (to avoid data leakage)\n",
    "        X_CV_train = Inp_db[Inp_db[index_col].isin(Inp_db[index_col].unique()[CV_train_idx])]\n",
    "        X_CV_test = Inp_db[Inp_db[index_col].isin(Inp_db[index_col].unique()[CV_test_idx])]\n",
    "        \n",
    "        Y_CV_train = X_CV_train[target_col].values\n",
    "        Y_CV_test = X_CV_test[target_col].values\n",
    "        \n",
    "        ## Extract the required features\n",
    "        X_CV_train_feat = X_CV_train[features]\n",
    "        X_CV_test_feat = X_CV_test[features]\n",
    "        \n",
    "        try:\n",
    "            Model.fit(X_CV_train_feat,Y_CV_train,eval_set=(X_CV_test[features],Y_CV_test))\n",
    "        except:\n",
    "            Model.fit(X_CV_train_feat,Y_CV_train)\n",
    "        \n",
    "        Y_CV_predict_test = Model.predict(X_CV_test_feat)\n",
    "        Y_CV_predict_train = Model.predict(X_CV_train_feat)\n",
    "        y_cv_train_pred = Model.predict_proba(X_CV_train_feat)[:,1]\n",
    "        y_cv_test_pred = Model.predict_proba(X_CV_test_feat)[:,1]\n",
    "        \n",
    "        train_results = classification_report(Y_CV_train,Model.predict(X_CV_train_feat),output_dict=True)\n",
    "        test_results = classification_report(Y_CV_test,Model.predict(X_CV_test_feat),output_dict=True)\n",
    "        \n",
    "        scores_cv_train[\"AUC\"]=np.append(scores_cv_train[\"AUC\"],roc_auc_score(Y_CV_train,y_cv_train_pred))\n",
    "        scores_cv_test[\"AUC\"]=np.append(scores_cv_test[\"AUC\"],roc_auc_score(Y_CV_test,y_cv_test_pred))\n",
    "\n",
    "        scores_cv_train[\"MCC\"]=np.append(scores_cv_train[\"MCC\"],matthews_corrcoef(Y_CV_train,Model.predict(X_CV_train_feat)))\n",
    "        scores_cv_test[\"MCC\"]=np.append(scores_cv_test[\"MCC\"],matthews_corrcoef(Y_CV_test,Model.predict(X_CV_test_feat)))\n",
    "        \n",
    "        scores_cv_train[\"ACCURACY\"]=np.append(scores_cv_train[\"ACCURACY\"],train_results[\"accuracy\"])\n",
    "        scores_cv_test[\"ACCURACY\"]=np.append(scores_cv_test[\"ACCURACY\"],test_results[\"accuracy\"])\n",
    "\n",
    "        scores_cv_train[\"RECALL\"]=np.append(scores_cv_train[\"RECALL\"],train_results['weighted avg']['recall'])\n",
    "        scores_cv_test[\"RECALL\"]=np.append(scores_cv_test[\"RECALL\"],test_results['weighted avg']['recall'])\n",
    "\n",
    "        scores_cv_train[\"F1\"]=np.append(scores_cv_train[\"F1\"],train_results['weighted avg']['f1-score'])\n",
    "        scores_cv_test[\"F1\"]=np.append(scores_cv_test[\"F1\"],test_results['weighted avg']['f1-score'])\n",
    "\n",
    "        scores_cv_train[\"PRECISION\"]=np.append(scores_cv_train[\"PRECISION\"],train_results['weighted avg']['precision'])\n",
    "        scores_cv_test[\"PRECISION\"]=np.append(scores_cv_test[\"PRECISION\"],test_results['weighted avg']['precision'])\n",
    "\n",
    "    #for key in scores_cv_train:\n",
    "    #    scores_cv_train[key]=[np.mean(scores_cv_train[key][0]),np.std(scores_cv_train[key][0])]\n",
    "    #    scores_cv_test[key]=[np.mean(scores_cv_test[key][0]),np.std(scores_cv_test[key][0])]\n",
    "\n",
    "    return scores_cv_train,scores_cv_test\n",
    "\n",
    "## This cross_validation is different from the standard cross validation, because the a priori dataset is only tested on actual a priori samples\n",
    "# Inp_db:pandas_DataFrame = input database (contains labels and features), should contain heroi2c numbers as IDs, previous concentrations\n",
    "# Features:List = features to use\n",
    "# Folds:int = amount of folds\n",
    "# RS:int = Random state\n",
    "# Output:dict = dictionary with the results of each fold\n",
    "def benchmark_regression_cross_validation(Model,Inp_db,index_col,folds,RS,features,target_col,disable_tqdm_output=False):\n",
    "    kf = KFold(n_splits=folds,shuffle=True,random_state=RS)\n",
    "\n",
    "    scores_cv_train = {\"R2\":np.array([]),\n",
    "                 \"RMSE\":np.array([]),      \n",
    "                 \"MAE\":np.array([]),\n",
    "                 \"ME\":np.array([])}\n",
    "    \n",
    "    scores_cv_test = {\"R2\":np.array([]),\n",
    "                 \"RMSE\":np.array([]),      \n",
    "                 \"MAE\":np.array([]),\n",
    "                 \"ME\":np.array([])}\n",
    "    \n",
    "    for CV_train_idx, CV_test_idx in tqdm(kf.split(Inp_db[index_col].unique()),disable=disable_tqdm_output): \n",
    "        ## Split per patient (to avoid data leakage)\n",
    "        X_CV_train = Inp_db[Inp_db[index_col].isin(Inp_db[index_col].unique()[CV_train_idx])]\n",
    "        X_CV_test = Inp_db[Inp_db[index_col].isin(Inp_db[index_col].unique()[CV_test_idx])]\n",
    "        \n",
    "        Y_CV_train = X_CV_train[target_col].values\n",
    "        Y_CV_test = X_CV_test[target_col].values\n",
    "        \n",
    "        ## Extract the required features\n",
    "        X_CV_train_feat = X_CV_train[features]\n",
    "        X_CV_test_feat = X_CV_test[features]\n",
    "        \n",
    "        try:\n",
    "            Model.fit(X_CV_train_feat,Y_CV_train,eval_set=(X_CV_test[features],Y_CV_test))\n",
    "        except:\n",
    "            Model.fit(X_CV_train_feat,Y_CV_train)\n",
    "        \n",
    "        Y_CV_predict_test = Model.predict(X_CV_test_feat)\n",
    "        Y_CV_predict_train = Model.predict(X_CV_train_feat)\n",
    "        \n",
    "        train_results = scores_calc_print(Y_CV_train,Model.predict(X_CV_train_feat),print_bool=False)\n",
    "        test_results = scores_calc_print(Y_CV_test,Model.predict(X_CV_test_feat),print_bool=False)\n",
    "        \n",
    "        scores_cv_train[\"R2\"]=np.append(scores_cv_train[\"R2\"],train_results[\"R2\"])\n",
    "        scores_cv_test[\"R2\"]=np.append(scores_cv_test[\"R2\"],test_results[\"R2\"])\n",
    "\n",
    "        scores_cv_train[\"RMSE\"]=np.append(scores_cv_train[\"RMSE\"],train_results[\"RMSE\"])\n",
    "        scores_cv_test[\"RMSE\"]=np.append(scores_cv_test[\"RMSE\"],test_results[\"RMSE\"])\n",
    "        \n",
    "        scores_cv_train[\"MAE\"]=np.append(scores_cv_train[\"MAE\"],train_results[\"MAE\"])\n",
    "        scores_cv_test[\"MAE\"]=np.append(scores_cv_test[\"MAE\"],test_results[\"MAE\"])\n",
    "\n",
    "        scores_cv_train[\"ME\"]=np.append(scores_cv_train[\"ME\"],train_results['ME'])\n",
    "        scores_cv_test[\"ME\"]=np.append(scores_cv_test[\"ME\"],test_results['ME'])\n",
    "\n",
    "    #for key in scores_cv_train:\n",
    "    #    scores_cv_train[key]=[np.mean(scores_cv_train[key][0]),np.std(scores_cv_train[key][0])]\n",
    "    #    scores_cv_test[key]=[np.mean(scores_cv_test[key][0]),np.std(scores_cv_test[key][0])]\n",
    "\n",
    "    return scores_cv_train,scores_cv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "def classification_forward_feature_selection(Model,Inp_db,index_col,folds,RS,features,target_col,metric):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    MAEs_train = []  \n",
    "    MAEs_test = []\n",
    "    Metrics_best = {\"AUC\":0.5,\"MCC\":0,\"ACCURACY\":0,\"RECALL\":0,\"F1\":0,\"PRECISION\":0}\n",
    "    temp_mean_sc = {\"AUC\":0.5,\"MCC\":0,\"ACCURACY\":0,\"RECALL\":0,\"F1\":0,\"PRECISION\":0}\n",
    "    Metric_changed = True\n",
    "\n",
    "    CV_features_all = features\n",
    "    CV_features_current_best = []\n",
    "    CV_features_arr_final = []\n",
    "\n",
    "    CV_RS = RS\n",
    "\n",
    "    #Forward feature selection method\n",
    "    while Metric_changed:\n",
    "        print(60*\"=\")\n",
    "        print(\"Iteration to select the \"+str(len(CV_features_arr_final)+1)+\"th feature.\")\n",
    "\n",
    "        #To keep the feature selection going\n",
    "        Metric_changed = False\n",
    "\n",
    "        for cv_feature in tqdm(CV_features_all,ascii=True):#iterator_array,ascii=True):\n",
    "            if cv_feature not in CV_features_arr_final:\n",
    "                try:\n",
    "                    CV_features_current = CV_features_arr_final+[cv_feature]\n",
    "\n",
    "                    try:\n",
    "                        CB_model_cv = Model.copy()\n",
    "                    except:\n",
    "                        CB_model_cv = clone(Model) \n",
    "\n",
    "                    train_sc,test_sc = benchmark_classification_cross_validation(Model,Inp_db,\n",
    "                                                                                 index_col,folds,RS,\n",
    "                                                                                 CV_features_current,target_col,True)\n",
    "\n",
    "                    for key in test_sc:\n",
    "                        temp_mean_sc[key]=np.mean(test_sc[key])\n",
    "\n",
    "                    if temp_mean_sc[metric]>Metrics_best[metric]:\n",
    "                        for key in temp_mean_sc:\n",
    "                            Metrics_best[key]=temp_mean_sc[key]\n",
    "                        CV_features_current_best = CV_features_current\n",
    "                        Metric_changed = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"Skipping this feature\")\n",
    "\n",
    "        CV_features_arr_final = CV_features_current_best\n",
    "        print(\"Current features: \"+str(CV_features_arr_final))\n",
    "        print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "    print(60*\"=\")\n",
    "    print(\"The best metrics are: \"+str(Metrics_best)+\" with the features: \"+str(CV_features_arr_final)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return CV_features_arr_final,Metrics_best\n",
    "\n",
    "\n",
    "def regression_forward_feature_selection(Model,Inp_db,index_col,folds,RS,features,target_col,metric):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    MAEs_train = []  \n",
    "    MAEs_test = []\n",
    "    Metrics_best = {\"R2\":-10,\"RMSE\":9999,\"MAE\":9999,\"ME\":9999}\n",
    "    temp_mean_sc = {\"R2\":-10,\"RMSE\":9999,\"MAE\":9999,\"ME\":9999}\n",
    "\n",
    "    Metric_changed = True\n",
    "\n",
    "    CV_features_all = features\n",
    "    CV_features_current_best = []\n",
    "    CV_features_arr_final = []\n",
    "\n",
    "    CV_RS = RS\n",
    "\n",
    "    #Forward feature selection method\n",
    "    while Metric_changed:\n",
    "        print(60*\"=\")\n",
    "        print(\"Iteration to select the \"+str(len(CV_features_arr_final)+1)+\"th feature.\")\n",
    "\n",
    "        #To keep the feature selection going\n",
    "        Metric_changed = False\n",
    "\n",
    "        for cv_feature in tqdm(CV_features_all,ascii=True):#iterator_array,ascii=True):\n",
    "            if cv_feature not in CV_features_arr_final:\n",
    "                try:\n",
    "                    CV_features_current = CV_features_arr_final+[cv_feature]\n",
    "\n",
    "                    try:\n",
    "                        CB_model_cv = Model.copy()\n",
    "                    except:\n",
    "                        CB_model_cv = clone(Model) \n",
    "\n",
    "                    train_sc,test_sc = benchmark_regression_cross_validation(Model,Inp_db,index_col,folds,RS,CV_features_current,target_col,True)\n",
    "\n",
    "                    for key in test_sc:\n",
    "                        temp_mean_sc[key]=np.mean(test_sc[key])\n",
    "\n",
    "                    if metric == \"R2\":\n",
    "                        if temp_mean_sc[metric]>Metrics_best[metric]:\n",
    "                            for key in temp_mean_sc:\n",
    "                                Metrics_best[key]=temp_mean_sc[key]\n",
    "                            CV_features_current_best = CV_features_current\n",
    "                            Metric_changed = True\n",
    "                    else:\n",
    "                        if temp_mean_sc[metric]<Metrics_best[metric]:\n",
    "                            for key in temp_mean_sc:\n",
    "                                Metrics_best[key]=temp_mean_sc[key]\n",
    "                            CV_features_current_best = CV_features_current\n",
    "                            Metric_changed = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"Skipping this feature\")\n",
    "\n",
    "        CV_features_arr_final = CV_features_current_best\n",
    "        print(\"Current features: \"+str(CV_features_arr_final))\n",
    "        print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "    print(60*\"=\")\n",
    "    print(\"The best metrics are: \"+str(Metrics_best)+\" with the features: \"+str(CV_features_arr_final)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return CV_features_arr_final,Metrics_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def classification_backwards_feature_selection(Model,Inp_db,index_col,folds,RS,features,target_col,metric):\n",
    "    start_time = time.time()\n",
    "    MAEs_train = []  \n",
    "    MAEs_test = []\n",
    "    Metrics_best = {\"AUC\":0.5,\"MCC\":0,\"ACCURACY\":0,\"RECALL\":0,\"F1\":0,\"PRECISION\":0}\n",
    "    temp_mean_sc = {\"AUC\":0.5,\"MCC\":0,\"ACCURACY\":0,\"RECALL\":0,\"F1\":0,\"PRECISION\":0}\n",
    "    Metric_changed = True\n",
    "\n",
    "    CV_features_all = list(features)\n",
    "    CV_features_current_best = []\n",
    "    CV_features_arr_final = list(features)\n",
    "\n",
    "    CV_RS = RS\n",
    "    \n",
    "    print(\"Getting the best metrics\")\n",
    "        #first test         \n",
    "    train_sc,test_sc = benchmark_classification_cross_validation(Model,Inp_db,\n",
    "                                         index_col,folds,RS,\n",
    "                                         CV_features_all,target_col,True)\n",
    "    for key in test_sc:\n",
    "        Metrics_best[key]=np.mean(test_sc[key])\n",
    "    print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "    #Backwards feature selection method\n",
    "    while Metric_changed:\n",
    "        print(60*\"=\")\n",
    "        print(\"Iteration to delete the \"+str(len(CV_features_arr_final)+1)+\"th feature.\")\n",
    "\n",
    "        #To keep the feature selection going\n",
    "        Metric_changed = False\n",
    "\n",
    "        for cv_feature in tqdm(CV_features_all,ascii=True):#iterator_array,ascii=True):\n",
    "            if cv_feature in CV_features_arr_final:\n",
    "                CV_features_current = copy.deepcopy(CV_features_all)\n",
    "                CV_features_current.remove(cv_feature)\n",
    "\n",
    "                CB_model_cv = Model.copy()\n",
    "\n",
    "                train_sc,test_sc = benchmark_classification_cross_validation(Model,Inp_db,\n",
    "                                                                             index_col,folds,RS,\n",
    "                                                                             CV_features_current,target_col,True)\n",
    "\n",
    "                for key in test_sc:\n",
    "                    temp_mean_sc[key]=np.mean(test_sc[key])\n",
    "\n",
    "                if temp_mean_sc[metric]>Metrics_best[metric]:\n",
    "                    for key in temp_mean_sc:\n",
    "                        Metrics_best[key]=temp_mean_sc[key]\n",
    "                    CV_features_current_best = CV_features_current\n",
    "                    Metric_changed = True\n",
    "\n",
    "        CV_features_arr_final = CV_features_current_best\n",
    "        print(\"Current features: \"+str(CV_features_arr_final))\n",
    "        print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "    print(60*\"=\")\n",
    "    print(\"The best metrics are: \"+str(Metrics_best)+\" with the features: \"+str(CV_features_arr_final)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return CV_features_arr_final,Metrics_best\n",
    "\n",
    "\n",
    "def regression_backwards_feature_selection(Model,Inp_db,index_col,folds,RS,features,target_col,metric):\n",
    "    start_time = time.time()\n",
    "    MAEs_train = []  \n",
    "    MAEs_test = []\n",
    "    Metrics_best = {\"R2\":-10,\"RMSE\":9999,\"MAE\":9999,\"ME\":9999}\n",
    "    temp_mean_sc = {\"R2\":-10,\"RMSE\":9999,\"MAE\":9999,\"ME\":9999}\n",
    "    Metric_changed = True\n",
    "\n",
    "    CV_features_all = list(features)\n",
    "    CV_features_current_best = []\n",
    "    CV_features_arr_final = list(features)\n",
    "\n",
    "    CV_RS = RS\n",
    "    \n",
    "    #first test   \n",
    "    print(\"Getting the best metrics\")\n",
    "    train_sc,test_sc = benchmark_regression_cross_validation(Model,Inp_db,\n",
    "                                         index_col,folds,RS,\n",
    "                                         CV_features_all,target_col,True)\n",
    "    for key in test_sc:\n",
    "        Metrics_best[key]=np.mean(test_sc[key])\n",
    "    print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "\n",
    "    #Forward feature selection method\n",
    "    while Metric_changed:\n",
    "        print(60*\"=\")\n",
    "        print(\"Iteration to delete the \"+str(len(CV_features_arr_final)+1)+\"th feature.\")\n",
    "\n",
    "        #To keep the feature selection going\n",
    "        Metric_changed = False\n",
    "        \n",
    "    \n",
    "        for cv_feature in tqdm(CV_features_all,ascii=True):#iterator_array,ascii=True):\n",
    "            if cv_feature in CV_features_arr_final:\n",
    "                CV_features_current = copy.deepcopy(CV_features_all)\n",
    "                CV_features_current.remove(cv_feature)\n",
    "\n",
    "                CB_model_cv = Model.copy()\n",
    "\n",
    "                train_sc,test_sc = benchmark_regression_cross_validation(Model,Inp_db,\n",
    "                                                                             index_col,folds,RS,\n",
    "                                                                             CV_features_current,target_col,True)\n",
    "\n",
    "                for key in test_sc:\n",
    "                    temp_mean_sc[key]=np.mean(test_sc[key])\n",
    "\n",
    "                if temp_mean_sc[metric]>Metrics_best[metric]:\n",
    "                    for key in temp_mean_sc:\n",
    "                        Metrics_best[key]=temp_mean_sc[key]\n",
    "                    CV_features_current_best = CV_features_current\n",
    "                    Metric_changed = True\n",
    "\n",
    "        CV_features_arr_final = CV_features_current_best\n",
    "        print(\"Current features: \"+str(CV_features_arr_final))\n",
    "        print(\"Status update: The current best metrics are: \"+str(Metrics_best))\n",
    "\n",
    "    print(60*\"=\")\n",
    "    print(\"The best metrics are: \"+str(Metrics_best)+\" with the features: \"+str(CV_features_arr_final)) \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return CV_features_arr_final,Metrics_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@activationFunction: \"relu\",\"tanh\",\"sigmoid\"\n",
    "class NN_model(tf.keras.Model):\n",
    "    def __init__(self,Layer_structure,activationFunction=\"relu\"):\n",
    "        super(NN_model,self).__init__()\n",
    "\n",
    "        ff_shape = len(Layer_structure)\n",
    "        \n",
    "        self.ff_layers = []\n",
    "\n",
    "        for ff_layer_i in range(ff_shape):\n",
    "            self.ff_layers.append(tf.keras.layers.Dense(Layer_structure[ff_layer_i], name=\"ff_layer_\"+str(ff_layer_i) ,activation=activationFunction))\n",
    "\n",
    "        self.y = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"final_layer\")\n",
    "\n",
    "    def call(self,inputs):\n",
    "        ffik = self.ff_layers[0](inputs)\n",
    "        for ff_layer in self.ff_layers[1:]:\n",
    "            ffik = ff_layer(ffik)\n",
    "\n",
    "        return self.y(ffik)\n",
    "\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        return self.call(inputs)\n",
    "    \n",
    "#@activationFunction: \"relu\",\"tanh\",\"sigmoid\"\n",
    "def NN_model(NNinputs,Layer_structure,activationFunction=\"relu\"):\n",
    "    #pass the input into the first layer\n",
    "\n",
    "    ffik = tf.keras.layers.Dense(Layer_structure[0], activation=activationFunction)(NNinputs)\n",
    "    ffiks = [ffik]\n",
    "    for ff_layer_i in range(len(Layer_structure)-1):\n",
    "            ffiks.append(tf.keras.layers.Dense(Layer_structure[ff_layer_i+1], activation=activationFunction)(ffiks[-1]))\n",
    "\n",
    "    y = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"final_layer\")(ffiks[-1])\n",
    "\n",
    "    model = tf.keras.Model(inputs=NNinputs, outputs=y, name=\"NN_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "#@activationFunction: \"relu\",\"tanh\",\"sigmoid\"\n",
    "def regression_NN_model(NNinputs,Layer_structure,activationFunction=\"relu\"):\n",
    "    #pass the input into the first layer\n",
    "\n",
    "    ffik = tf.keras.layers.Dense(Layer_structure[0], activation=activationFunction)(NNinputs)\n",
    "    ffiks = [ffik]\n",
    "    for ff_layer_i in range(len(Layer_structure)-1):\n",
    "            ffiks.append(tf.keras.layers.Dense(Layer_structure[ff_layer_i+1], activation=activationFunction)(ffiks[-1]))\n",
    "\n",
    "    y = tf.keras.layers.Dense(1, activation=\"linear\",name=\"final_layer\")(ffiks[-1])\n",
    "\n",
    "    model = tf.keras.Model(inputs=NNinputs, outputs=y, name=\"NN_model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,classification_report,auc,r2_score,matthews_corrcoef\n",
    "\n",
    "## This cross_validation is different from the standard cross validation, because the a priori dataset is only tested on actual a priori samples\n",
    "# Inp_db:pandas_DataFrame = input database (contains labels and features), should contain heroi2c numbers as IDs, previous concentrations\n",
    "# Features:List = features to use\n",
    "# Folds:int = amount of folds\n",
    "# RS:int = Random state\n",
    "# Output:dict = dictionary with the results of each fold\n",
    "def test_bootstrap_eval_class(Model,test_df,bootstrap_its,RS,features,target_col,disable_tqdm_output=False):\n",
    "    scores_cv_test = {\"AUC\":np.array([]),\n",
    "                 \"MCC\":np.array([]),      \n",
    "                 \"ACCURACY\":np.array([]),\n",
    "                 \"RECALL\":np.array([]),\n",
    "                 \"F1\":np.array([]),\n",
    "                 \"PRECISION\":np.array([])}\n",
    "    \n",
    "    for i in tqdm(range(bootstrap_its)):\n",
    "        sampled_df = test_df.sample(n=int(0.66*len(test_df)),replace=True,random_state=i)\n",
    "\n",
    "        Y_CV_test = sampled_df[target_col].values\n",
    "        ## Extract the required features\n",
    "        X_CV_test_feat = sampled_df[features].values\n",
    "\n",
    "        Y_CV_predict_test = Model.predict(X_CV_test_feat)\n",
    "        y_cv_test_pred = Model.predict_proba(X_CV_test_feat)[:,1]\n",
    "\n",
    "        test_results = classification_report(Y_CV_test,Y_CV_predict_test,output_dict=True)\n",
    "\n",
    "        scores_cv_test[\"AUC\"]=np.append(scores_cv_test[\"AUC\"],roc_auc_score(Y_CV_test,y_cv_test_pred))\n",
    "        scores_cv_test[\"MCC\"]=np.append(scores_cv_test[\"MCC\"],matthews_corrcoef(Y_CV_test,Y_CV_predict_test))\n",
    "        scores_cv_test[\"ACCURACY\"]=np.append(scores_cv_test[\"ACCURACY\"],test_results[\"accuracy\"])\n",
    "        scores_cv_test[\"RECALL\"]=np.append(scores_cv_test[\"RECALL\"],test_results['weighted avg']['recall'])\n",
    "        scores_cv_test[\"F1\"]=np.append(scores_cv_test[\"F1\"],test_results['weighted avg']['f1-score'])\n",
    "        scores_cv_test[\"PRECISION\"]=np.append(scores_cv_test[\"PRECISION\"],test_results['weighted avg']['precision'])\n",
    "\n",
    "    return scores_cv_test\n",
    "\n",
    "## This cross_validation is different from the standard cross validation, because the a priori dataset is only tested on actual a priori samples\n",
    "# Inp_db:pandas_DataFrame = input database (contains labels and features), should contain heroi2c numbers as IDs, previous concentrations\n",
    "# Features:List = features to use\n",
    "# Folds:int = amount of folds\n",
    "# RS:int = Random state\n",
    "# Output:dict = dictionary with the results of each fold\n",
    "def test_bootstrap_eval_regres(Model,test_df,bootstrap_its,RS,features,target_col,disable_tqdm_output=False):\n",
    "\n",
    "    scores_cv_train = {\"R2\":np.array([]),\n",
    "                 \"RMSE\":np.array([]),      \n",
    "                 \"MAE\":np.array([]),\n",
    "                 \"ME\":np.array([])}\n",
    "    \n",
    "    scores_cv_test = {\"R2\":np.array([]),\n",
    "                 \"RMSE\":np.array([]),      \n",
    "                 \"MAE\":np.array([]),\n",
    "                 \"ME\":np.array([])}\n",
    "    \n",
    "    for i in tqdm(range(bootstrap_its)):\n",
    "        ## Split per patient (to avoid data leakage)\n",
    "        sampled_df = test_df.sample(n=int(0.66*len(test_df)),replace=True,random_state=i)\n",
    "\n",
    "        Y_CV_test = sampled_df[target_col].values\n",
    "        ## Extract the required features\n",
    "        X_CV_test_feat = sampled_df[features]\n",
    "\n",
    "        Y_CV_predict_test = Model.predict(X_CV_test_feat)\n",
    "        test_results = scores_calc_print(Y_CV_test,Model.predict(X_CV_test_feat),print_bool=False)\n",
    "        \n",
    "        scores_cv_test[\"R2\"]=np.append(scores_cv_test[\"R2\"],test_results[\"R2\"])\n",
    "        scores_cv_test[\"RMSE\"]=np.append(scores_cv_test[\"RMSE\"],test_results[\"RMSE\"])\n",
    "        scores_cv_test[\"MAE\"]=np.append(scores_cv_test[\"MAE\"],test_results[\"MAE\"])\n",
    "        scores_cv_test[\"ME\"]=np.append(scores_cv_test[\"ME\"],test_results['ME'])\n",
    "\n",
    "    return scores_cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADELON dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = pd.read_csv(\"data/madelon.csv\")\n",
    "current_df = current_df.reset_index()\n",
    "current_df.loc[current_df.Class==0,\"Class\"]=-1#0\n",
    "train_idx,val_idx = train_test_split(current_df[\"index\"],test_size=0.25,random_state = 1)\n",
    "current_db_train = current_df[current_df[\"index\"].isin(train_idx)]\n",
    "current_db_test = current_df[current_df[\"index\"].isin(val_idx)]\n",
    "\n",
    "target_col = \"Class\"\n",
    "Index_col = \"index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powershap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting powershap\n",
      "Automatic mode enabled: Finding the minimal required powershap iterations for significance of 0.01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6d71c5a899448ba7123b4dbee617f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 18 iterations; Adding 8  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24080290649b44e0ac0fde1a6e543f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "selector = PowerSHAP(\n",
    "    model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,target_col=target_col,index_col=Index_col,\n",
    ")\n",
    "selector.fit(current_db_train.drop(columns=[Index_col,target_col]), current_db_train[target_col])\n",
    "t = selector._processed_shaps_df\n",
    "t.reset_index().to_csv(\"results/madelon_PowerSHAP_catboost_results_automatic_mode.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borutashap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=False,iterations=250)#,use_best_model=True)\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True)\n",
    "\n",
    "Feature_Selector.fit(X=current_db_train[list(current_db_train.columns.values[1:-1])], y=current_db_train[target_col], sample=False,\n",
    "                        train_or_test = 'test', normalize=True,verbose=True)\n",
    "subset = Feature_Selector.Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [10:32<00:00,  6.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['V5', 'V29', 'V49', 'V65', 'V106', 'V129', 'V154', 'V198', 'V205',\n",
       "       'V242', 'V249', 'V282', 'V283', 'V286', 'V305', 'V307', 'V319',\n",
       "       'V337', 'V339', 'V379', 'V425', 'V434', 'V443', 'V452', 'V454',\n",
       "       'V456', 'V472', 'V473', 'V476', 'V494'], dtype='<U4')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inp_db = current_db_train.copy(deep=True)\n",
    "train_idx,val_idx = train_test_split(Inp_db[Index_col],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=True)\n",
    "CV_features_arr_final,Metrics_best = classification_forward_feature_selection(shap_model,current_db_train,Index_col,5,0,current_db_train.drop(columns=[target_col,Index_col]).columns.values,target_col,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TRAIN\n",
      "AUC: 0.999 (0.0)\n",
      "MCC: 0.976 (0.009)\n",
      "ACCURACY: 0.988 (0.004)\n",
      "RECALL: 0.988 (0.004)\n",
      "F1: 0.988 (0.004)\n",
      "PRECISION: 0.988 (0.004)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.947 (0.017)\n",
      "MCC: 0.764 (0.046)\n",
      "ACCURACY: 0.881 (0.023)\n",
      "RECALL: 0.881 (0.023)\n",
      "F1: 0.881 (0.023)\n",
      "PRECISION: 0.883 (0.024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"powershap\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['V339', 'V242', 'V379', 'V29', 'V456', 'V282', 'V494', 'V129'] #forward feature selection on AUC\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['V434', 'V242', 'V379', 'V5', 'V476', 'V49', 'V282', 'V286', 'V339','V29'] #borutoSHAP \n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    processed_shaps_df = pd.read_csv(\"results/madelon_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['V5', 'V29', 'V49', 'V65', 'V106', 'V129', 'V154', 'V198', 'V205',\n",
    "           'V242', 'V249', 'V282', 'V283', 'V286', 'V305', 'V307', 'V319',\n",
    "           'V337', 'V339', 'V379', 'V425', 'V434', 'V443', 'V452', 'V454',\n",
    "           'V456', 'V472', 'V473', 'V476', 'V494']\n",
    "\n",
    "elif model ==\"chi\":\n",
    "    #chi squared p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(chi2(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_df.columns.values[1:-1])\n",
    "    \n",
    "\n",
    "print(len(selected_features))\n",
    "\n",
    "CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2,use_best_model=True)\n",
    "\n",
    "scores_cv_train,scores_cv_test = benchmark_classification_cross_validation(Model = CB_model,Inp_db = current_db_train.copy(deep=True),index_col=Index_col,folds=10,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TRAIN\")\n",
    "for key in scores_cv_train:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_train[key]),3))+\" (\"+str(np.round(np.std(scores_cv_train[key]),3))+\")\")\n",
    "print(50*\"=\")\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:11<00:00, 87.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TEST\n",
      "AUC: 0.95 (0.01)\n",
      "MCC: 0.768 (0.031)\n",
      "ACCURACY: 0.883 (0.016)\n",
      "RECALL: 0.883 (0.016)\n",
      "F1: 0.883 (0.016)\n",
      "PRECISION: 0.885 (0.015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"powershap\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['V339', 'V242', 'V379', 'V29', 'V456', 'V282', 'V494', 'V129'] #forward feature selection on AUC\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['V434', 'V242', 'V379', 'V5', 'V476', 'V49', 'V282', 'V286', 'V339','V29'] #borutoSHAP \n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    #processed_shaps_df = pd.read_csv(\"results/madelon_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    #selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "    \n",
    "    selected_features = t[t.p_value<0.01].index.values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['V5', 'V29', 'V49', 'V65', 'V106', 'V129', 'V154', 'V198', 'V205',\n",
    "           'V242', 'V249', 'V282', 'V283', 'V286', 'V305', 'V307', 'V319',\n",
    "           'V337', 'V339', 'V379', 'V425', 'V434', 'V443', 'V452', 'V454',\n",
    "           'V456', 'V472', 'V473', 'V476', 'V494']\n",
    "\n",
    "elif model ==\"chi\":\n",
    "    #chi squared p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(chi2(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_df.columns.values[1:-1])\n",
    "\n",
    "X_train = current_db_train[selected_features]\n",
    "Y_train = current_db_train[target_col]\n",
    "\n",
    "X_test = current_db_test[selected_features]\n",
    "Y_test = current_db_test[target_col]\n",
    "\n",
    "CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2)#,per_float_feature_quantization=['1:border_count=1024'])\n",
    "CB_model.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "scores_cv_test = test_bootstrap_eval_class(Model = CB_model,test_df = current_db_test.copy(deep=True),bootstrap_its=1000,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GINA PRIORI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "gina_prior_df = pd.read_csv(\"data/gina_prior.csv\")\n",
    "gina_prior_df = gina_prior_df.reset_index()\n",
    "gina_prior_df.loc[gina_prior_df.label==-1,\"label\"]=0\n",
    "train_idx,val_idx = train_test_split(gina_prior_df[\"index\"],test_size=0.25,random_state = 1)\n",
    "current_db_train = gina_prior_df[gina_prior_df[\"index\"].isin(train_idx)]\n",
    "current_db_test = gina_prior_df[gina_prior_df[\"index\"].isin(val_idx)]\n",
    "\n",
    "target_col = \"label\"\n",
    "Index_col = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powershap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting powershap\n",
      "Automatic mode enabled: Finding the minimal required powershap iterations for significance of 0.01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2de4e6705cc4efe81232b8e618717d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 16 iterations; Adding 6  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ae9b087804d7bae3a005a7ce829e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 22 iterations; Adding 6  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7df820352f4e1298ce284b3e7fe4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "selector = PowerSHAP(\n",
    "    model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,target_col=target_col,index_col=Index_col,\n",
    ")\n",
    "selector.fit(current_db_train.drop(columns=[Index_col,target_col]), current_db_train[target_col])\n",
    "t = selector._processed_shaps_df\n",
    "t.reset_index().to_csv(\"results/gina_prior_PowerSHAP_catboost_results_automatic_mode.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borutashap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=False,iterations=250)#,use_best_model=True)\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True)\n",
    "\n",
    "Feature_Selector.fit(X=current_db_train[list(current_db_train.columns.values[1:-1])], y=current_db_train[target_col], sample=False,\n",
    "                        train_or_test = 'test', normalize=True,verbose=True)\n",
    "subset = Feature_Selector.Subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [13:32<00:00,  8.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['pixel103', 'pixel104', 'pixel137', 'pixel152', 'pixel153',\n",
       "       'pixel154', 'pixel157', 'pixel158', 'pixel184', 'pixel211',\n",
       "       'pixel212', 'pixel213', 'pixel238', 'pixel239', 'pixel240',\n",
       "       'pixel241', 'pixel242', 'pixel243', 'pixel249', 'pixel251',\n",
       "       'pixel267', 'pixel268', 'pixel269', 'pixel295', 'pixel296',\n",
       "       'pixel297', 'pixel319', 'pixel323', 'pixel324', 'pixel347',\n",
       "       'pixel348', 'pixel351', 'pixel352', 'pixel358', 'pixel359',\n",
       "       'pixel376', 'pixel377', 'pixel383', 'pixel387', 'pixel403',\n",
       "       'pixel404', 'pixel405', 'pixel410', 'pixel414', 'pixel416',\n",
       "       'pixel427', 'pixel428', 'pixel429', 'pixel432', 'pixel438',\n",
       "       'pixel454', 'pixel455', 'pixel456', 'pixel458', 'pixel459',\n",
       "       'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel465',\n",
       "       'pixel483', 'pixel484', 'pixel485', 'pixel487', 'pixel488',\n",
       "       'pixel489', 'pixel490', 'pixel498', 'pixel500', 'pixel511',\n",
       "       'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517',\n",
       "       'pixel518', 'pixel528', 'pixel540', 'pixel541', 'pixel543',\n",
       "       'pixel544', 'pixel545', 'pixel546', 'pixel548', 'pixel569',\n",
       "       'pixel572', 'pixel573', 'pixel576', 'pixel579', 'pixel581',\n",
       "       'pixel584', 'pixel585', 'pixel604', 'pixel607', 'pixel611',\n",
       "       'pixel626', 'pixel627', 'pixel630', 'pixel635', 'pixel680',\n",
       "       'pixel709', 'pixel713', 'pixel714', 'pixel716', 'pixel718',\n",
       "       'pixel719'], dtype='<U8')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inp_db = current_db_train.copy(deep=True)\n",
    "train_idx,val_idx = train_test_split(Inp_db[Index_col],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True).drop(columns=[Index_col,target_col])\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=True)\n",
    "CV_features_arr_final,Metrics_best = classification_forward_feature_selection(shap_model,current_db_train,Index_col,5,0,current_db_train.drop(columns=[target_col,Index_col]).columns.values,target_col,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pixel240', 'pixel488', 'pixel513', 'pixel514', 'pixel456',\n",
       "       'pixel515', 'pixel544', 'pixel516', 'pixel239', 'pixel543',\n",
       "       'pixel323', 'pixel351', 'pixel486', 'pixel324', 'pixel268',\n",
       "       'pixel485', 'pixel487', 'pixel458', 'pixel455', 'pixel484',\n",
       "       'pixel212', 'pixel463', 'pixel211', 'pixel213', 'pixel460',\n",
       "       'pixel548', 'pixel241', 'pixel296', 'pixel352', 'pixel489',\n",
       "       'pixel483', 'pixel546', 'pixel267', 'pixel457', 'pixel403',\n",
       "       'pixel573', 'pixel238', 'pixel157', 'pixel517', 'pixel459',\n",
       "       'pixel295', 'pixel269', 'pixel404', 'pixel156', 'pixel490',\n",
       "       'pixel462', 'pixel511', 'pixel375', 'pixel379', 'pixel432',\n",
       "       'pixel713', 'pixel242', 'pixel542', 'pixel243', 'pixel410',\n",
       "       'pixel630', 'pixel429', 'pixel571', 'pixel604', 'pixel545',\n",
       "       'pixel377', 'pixel376', 'pixel297', 'pixel461', 'pixel518',\n",
       "       'pixel570', 'pixel431', 'pixel540', 'pixel491', 'pixel348',\n",
       "       'pixel433', 'pixel465', 'pixel414', 'pixel576', 'pixel541',\n",
       "       'pixel572', 'pixel184', 'pixel464', 'pixel382', 'pixel579',\n",
       "       'pixel438', 'pixel680', 'pixel606', 'pixel405', 'pixel430',\n",
       "       'pixel185', 'pixel325', 'pixel411', 'pixel492', 'pixel437',\n",
       "       'pixel436', 'pixel626', 'pixel659', 'pixel657', 'pixel152',\n",
       "       'pixel409', 'pixel214', 'pixel353', 'pixel718', 'pixel435',\n",
       "       'pixel263', 'pixel628', 'pixel600', 'pixel466', 'pixel356'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[t.p_value<0.01].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:23,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TRAIN\n",
      "AUC: 1.0 (0.0)\n",
      "MCC: 1.0 (0.001)\n",
      "ACCURACY: 1.0 (0.0)\n",
      "RECALL: 1.0 (0.0)\n",
      "F1: 1.0 (0.0)\n",
      "PRECISION: 1.0 (0.0)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.99 (0.004)\n",
      "MCC: 0.906 (0.023)\n",
      "ACCURACY: 0.953 (0.011)\n",
      "RECALL: 0.953 (0.011)\n",
      "F1: 0.953 (0.011)\n",
      "PRECISION: 0.953 (0.011)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"powershap\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['pixel514', 'pixel324', 'pixel455', 'pixel240', 'pixel544', \n",
    "               'pixel626', 'pixel460', 'pixel154', 'pixel211', 'pixel266', \n",
    "               'pixel457', 'pixel436', 'pixel376', 'pixel383', 'pixel490', 'pixel540', 'pixel242', 'pixel636', \n",
    "               'pixel550', 'pixel630', 'pixel301', 'pixel158', 'pixel627', 'pixel267', 'pixel458', 'pixel223'] #forward feature selection on AUC\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['pixel543', 'pixel296', 'pixel157', 'pixel455', 'pixel515', 'pixel490', 'pixel352', 'pixel548', 'pixel488', \n",
    "                         'pixel324', 'pixel211', 'pixel351', 'pixel239', 'pixel713', 'pixel269', 'pixel489', 'pixel516', 'pixel460', \n",
    "                         'pixel212', 'pixel513', 'pixel463', 'pixel457', 'pixel514', \n",
    "                         'pixel240', 'pixel241', 'pixel267', 'pixel573', 'pixel487', 'pixel486', 'pixel484', 'pixel268', 'pixel511', \n",
    "                         'pixel485', 'pixel544', 'pixel456', 'pixel213', 'pixel323'] #borutoSHAP \n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    processed_shaps_df = pd.read_csv(\"results/gina_prior_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['pixel103', 'pixel104', 'pixel137', 'pixel152', 'pixel153',\n",
    "       'pixel154', 'pixel157', 'pixel158', 'pixel184', 'pixel211',\n",
    "       'pixel212', 'pixel213', 'pixel238', 'pixel239', 'pixel240',\n",
    "       'pixel241', 'pixel242', 'pixel243', 'pixel249', 'pixel251',\n",
    "       'pixel267', 'pixel268', 'pixel269', 'pixel295', 'pixel296',\n",
    "       'pixel297', 'pixel319', 'pixel323', 'pixel324', 'pixel347',\n",
    "       'pixel348', 'pixel351', 'pixel352', 'pixel358', 'pixel359',\n",
    "       'pixel376', 'pixel377', 'pixel383', 'pixel387', 'pixel403',\n",
    "       'pixel404', 'pixel405', 'pixel410', 'pixel414', 'pixel416',\n",
    "       'pixel427', 'pixel428', 'pixel429', 'pixel432', 'pixel438',\n",
    "       'pixel454', 'pixel455', 'pixel456', 'pixel458', 'pixel459',\n",
    "       'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel465',\n",
    "       'pixel483', 'pixel484', 'pixel485', 'pixel487', 'pixel488',\n",
    "       'pixel489', 'pixel490', 'pixel498', 'pixel500', 'pixel511',\n",
    "       'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517',\n",
    "       'pixel518', 'pixel528', 'pixel540', 'pixel541', 'pixel543',\n",
    "       'pixel544', 'pixel545', 'pixel546', 'pixel548', 'pixel569',\n",
    "       'pixel572', 'pixel573', 'pixel576', 'pixel579', 'pixel581',\n",
    "       'pixel584', 'pixel585', 'pixel604', 'pixel607', 'pixel611',\n",
    "       'pixel626', 'pixel627', 'pixel630', 'pixel635', 'pixel680',\n",
    "       'pixel709', 'pixel713', 'pixel714', 'pixel716', 'pixel718',\n",
    "       'pixel719']\n",
    "\n",
    "elif model ==\"chi\":\n",
    "    #chi squared p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(chi2(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[1:-1]) \n",
    "\n",
    "print(len(selected_features))\n",
    "\n",
    "CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2,use_best_model=True)\n",
    "\n",
    "scores_cv_train,scores_cv_test = benchmark_classification_cross_validation(Model = CB_model,Inp_db = current_db_train.copy(deep=True),index_col=Index_col,folds=10,RS=0,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TRAIN\")\n",
    "for key in scores_cv_train:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_train[key]),3))+\" (\"+str(np.round(np.std(scores_cv_train[key]),3))+\")\")\n",
    "print(50*\"=\")\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:13<00:00, 74.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "AUC: 0.986 (0.004)\n",
      "MCC: 0.896 (0.019)\n",
      "ACCURACY: 0.948 (0.009)\n",
      "RECALL: 0.948 (0.009)\n",
      "F1: 0.948 (0.009)\n",
      "PRECISION: 0.948 (0.009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"shapicant\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['pixel514', 'pixel324', 'pixel455', 'pixel240', 'pixel544', \n",
    "               'pixel626', 'pixel460', 'pixel154', 'pixel211', 'pixel266', \n",
    "               'pixel457', 'pixel436', 'pixel376', 'pixel383', 'pixel490', 'pixel540', 'pixel242', 'pixel636', \n",
    "               'pixel550', 'pixel630', 'pixel301', 'pixel158', 'pixel627', 'pixel267', 'pixel458', 'pixel223'] #forward feature selection on AUC\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['pixel543', 'pixel296', 'pixel157', 'pixel455', 'pixel515', 'pixel490', 'pixel352', 'pixel548', 'pixel488', \n",
    "                         'pixel324', 'pixel211', 'pixel351', 'pixel239', 'pixel713', 'pixel269', 'pixel489', 'pixel516', 'pixel460', \n",
    "                         'pixel212', 'pixel513', 'pixel463', 'pixel457', 'pixel514', \n",
    "                         'pixel240', 'pixel241', 'pixel267', 'pixel573', 'pixel487', 'pixel486', 'pixel484', 'pixel268', 'pixel511', \n",
    "                         'pixel485', 'pixel544', 'pixel456', 'pixel213', 'pixel323'] #borutoSHAP \n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    processed_shaps_df = pd.read_csv(\"results/gina_prior_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['pixel103', 'pixel104', 'pixel137', 'pixel152', 'pixel153',\n",
    "       'pixel154', 'pixel157', 'pixel158', 'pixel184', 'pixel211',\n",
    "       'pixel212', 'pixel213', 'pixel238', 'pixel239', 'pixel240',\n",
    "       'pixel241', 'pixel242', 'pixel243', 'pixel249', 'pixel251',\n",
    "       'pixel267', 'pixel268', 'pixel269', 'pixel295', 'pixel296',\n",
    "       'pixel297', 'pixel319', 'pixel323', 'pixel324', 'pixel347',\n",
    "       'pixel348', 'pixel351', 'pixel352', 'pixel358', 'pixel359',\n",
    "       'pixel376', 'pixel377', 'pixel383', 'pixel387', 'pixel403',\n",
    "       'pixel404', 'pixel405', 'pixel410', 'pixel414', 'pixel416',\n",
    "       'pixel427', 'pixel428', 'pixel429', 'pixel432', 'pixel438',\n",
    "       'pixel454', 'pixel455', 'pixel456', 'pixel458', 'pixel459',\n",
    "       'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel465',\n",
    "       'pixel483', 'pixel484', 'pixel485', 'pixel487', 'pixel488',\n",
    "       'pixel489', 'pixel490', 'pixel498', 'pixel500', 'pixel511',\n",
    "       'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517',\n",
    "       'pixel518', 'pixel528', 'pixel540', 'pixel541', 'pixel543',\n",
    "       'pixel544', 'pixel545', 'pixel546', 'pixel548', 'pixel569',\n",
    "       'pixel572', 'pixel573', 'pixel576', 'pixel579', 'pixel581',\n",
    "       'pixel584', 'pixel585', 'pixel604', 'pixel607', 'pixel611',\n",
    "       'pixel626', 'pixel627', 'pixel630', 'pixel635', 'pixel680',\n",
    "       'pixel709', 'pixel713', 'pixel714', 'pixel716', 'pixel718',\n",
    "       'pixel719']\n",
    "\n",
    "elif model ==\"chi\":\n",
    "    #chi squared p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(chi2(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[1:-1][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[1:-1]) \n",
    "    \n",
    "\n",
    "X_train = current_db_train[selected_features]\n",
    "Y_train = current_db_train[target_col]\n",
    "\n",
    "X_test = current_db_test[selected_features]\n",
    "Y_test = current_db_test[target_col]\n",
    "\n",
    "CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2)#,per_float_feature_quantization=['1:border_count=1024'])\n",
    "#CB_model = LogisticRegression()\n",
    "#CB_model = RandomForestClassifier()#verbose=False,iterations=250,random_seed=2,use_best_model=True)\n",
    "CB_model.fit(X_train,Y_train)\n",
    "        \n",
    "scores_cv_test = test_bootstrap_eval_class(Model = CB_model,test_df = current_db_test.copy(deep=True),bootstrap_its=1000,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Class SCENE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db = pd.read_csv(\"data/scene.csv\")\n",
    "current_db = current_db.reset_index()\n",
    "\n",
    "Index_col = \"index\"\n",
    "target_col = \"Urban\"\n",
    "\n",
    "current_db[target_col]=current_db[target_col].astype(np.int32)\n",
    "\n",
    "train_idx,val_idx = train_test_split(current_db[Index_col],test_size=0.25,random_state = 1,stratify=current_db[target_col])\n",
    "current_db_train = current_db[current_db[Index_col].isin(train_idx)]\n",
    "current_db_test = current_db[current_db[Index_col].isin(val_idx)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powershap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting powershap\n",
      "Automatic mode enabled: Finding the minimal required powershap iterations for significance of 0.01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c22e62c3d74896bf7af2ca794a38d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: Powershap requires 16 iterations; Adding 6  powershap iterations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d658604455440c82849a033917d378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selector = PowerSHAP(\n",
    "    model = CatBoostClassifier(verbose=0, n_estimators=250,use_best_model=True,class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)]),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,stratify=True,force_convergence = True,\n",
    ")\n",
    "selector.fit(current_db_train[list(current_db_train.columns.values[1:-6])], current_db_train[target_col])\n",
    "t = selector._processed_shaps_df\n",
    "#t.reset_index().to_csv(\"results/scene_PowerSHAP_catboost_results_automatic_mode.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borutashap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=False,iterations=250,class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)])\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True)\n",
    "\n",
    "Feature_Selector.fit(X=current_db_train[l/ist(current_db_train.columns.values[1:-6])], y=current_db_train[target_col], sample=False,\n",
    "                        train_or_test = 'test', normalize=True,verbose=True)\n",
    "subset = Feature_Selector.Subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [12:29<00:00,  7.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Att15', 'Att17', 'Att18', 'Att20', 'Att22', 'Att23', 'Att27',\n",
       "       'Att40', 'Att44', 'Att45', 'Att47', 'Att48', 'Att49', 'Att53',\n",
       "       'Att68', 'Att72', 'Att78', 'Att80', 'Att82', 'Att83', 'Att86',\n",
       "       'Att87', 'Att89', 'Att91', 'Att98', 'Att103', 'Att106', 'Att108',\n",
       "       'Att118', 'Att132', 'Att133', 'Att141', 'Att171', 'Att185',\n",
       "       'Att195', 'Att200', 'Att201', 'Att204', 'Att205', 'Att207',\n",
       "       'Att208', 'Att209', 'Att212', 'Att222', 'Att223', 'Att226',\n",
       "       'Att228', 'Att229', 'Att237', 'Att239', 'Att240', 'Att241',\n",
       "       'Att242', 'Att245', 'Att253', 'Att269'], dtype='<U6')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inp_db = current_db_train.copy(deep=True)\n",
    "train_idx,val_idx = train_test_split(Inp_db[Index_col],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True)[list(current_db_train.columns.values[1:-6])]\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True)[list(current_db_train.columns.values[1:-6])]\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostClassifier(verbose=False,iterations=250,use_best_model=False,class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)])\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_model = CatBoostClassifier(verbose=False,iterations=250,class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)],use_best_model=True)\n",
    "\n",
    "CV_features_arr_final,Metrics_best = classification_forward_feature_selection(shap_model,current_db_train,Index_col,5,0,list(current_db_train.columns.values[1:-6]),target_col,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.52it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "TRAIN\n",
      "AUC: 0.991 (0.01)\n",
      "MCC: 0.853 (0.08)\n",
      "ACCURACY: 0.947 (0.032)\n",
      "RECALL: 0.947 (0.032)\n",
      "F1: 0.95 (0.029)\n",
      "PRECISION: 0.961 (0.02)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.927 (0.032)\n",
      "MCC: 0.643 (0.09)\n",
      "ACCURACY: 0.878 (0.039)\n",
      "RECALL: 0.878 (0.039)\n",
      "F1: 0.884 (0.036)\n",
      "PRECISION: 0.898 (0.03)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "borutashap\n",
      "TRAIN\n",
      "AUC: 0.981 (0.014)\n",
      "MCC: 0.787 (0.066)\n",
      "ACCURACY: 0.921 (0.028)\n",
      "RECALL: 0.921 (0.028)\n",
      "F1: 0.926 (0.025)\n",
      "PRECISION: 0.943 (0.017)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.902 (0.03)\n",
      "MCC: 0.573 (0.078)\n",
      "ACCURACY: 0.848 (0.032)\n",
      "RECALL: 0.848 (0.032)\n",
      "F1: 0.858 (0.03)\n",
      "PRECISION: 0.88 (0.028)\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:09,  1.02it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TRAIN\n",
      "AUC: 0.995 (0.004)\n",
      "MCC: 0.873 (0.06)\n",
      "ACCURACY: 0.955 (0.023)\n",
      "RECALL: 0.955 (0.023)\n",
      "F1: 0.958 (0.021)\n",
      "PRECISION: 0.966 (0.016)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.927 (0.022)\n",
      "MCC: 0.624 (0.076)\n",
      "ACCURACY: 0.872 (0.031)\n",
      "RECALL: 0.872 (0.031)\n",
      "F1: 0.878 (0.03)\n",
      "PRECISION: 0.892 (0.029)\n",
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:15,  1.52s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapicant\n",
      "TRAIN\n",
      "AUC: 0.998 (0.003)\n",
      "MCC: 0.922 (0.053)\n",
      "ACCURACY: 0.974 (0.019)\n",
      "RECALL: 0.974 (0.019)\n",
      "F1: 0.975 (0.018)\n",
      "PRECISION: 0.978 (0.014)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.937 (0.026)\n",
      "MCC: 0.65 (0.064)\n",
      "ACCURACY: 0.888 (0.03)\n",
      "RECALL: 0.888 (0.03)\n",
      "F1: 0.892 (0.028)\n",
      "PRECISION: 0.898 (0.024)\n",
      "93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:25,  2.53s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi\n",
      "TRAIN\n",
      "AUC: 0.99 (0.009)\n",
      "MCC: 0.829 (0.091)\n",
      "ACCURACY: 0.936 (0.036)\n",
      "RECALL: 0.936 (0.036)\n",
      "F1: 0.94 (0.033)\n",
      "PRECISION: 0.955 (0.024)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.911 (0.03)\n",
      "MCC: 0.583 (0.078)\n",
      "ACCURACY: 0.856 (0.037)\n",
      "RECALL: 0.856 (0.037)\n",
      "F1: 0.864 (0.034)\n",
      "PRECISION: 0.881 (0.026)\n",
      "220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:57,  5.75s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_test\n",
      "TRAIN\n",
      "AUC: 0.997 (0.006)\n",
      "MCC: 0.913 (0.069)\n",
      "ACCURACY: 0.97 (0.026)\n",
      "RECALL: 0.97 (0.026)\n",
      "F1: 0.971 (0.024)\n",
      "PRECISION: 0.976 (0.018)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.927 (0.028)\n",
      "MCC: 0.636 (0.066)\n",
      "ACCURACY: 0.88 (0.031)\n",
      "RECALL: 0.88 (0.031)\n",
      "F1: 0.885 (0.029)\n",
      "PRECISION: 0.894 (0.027)\n",
      "294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:12,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "TRAIN\n",
      "AUC: 0.998 (0.004)\n",
      "MCC: 0.92 (0.069)\n",
      "ACCURACY: 0.972 (0.025)\n",
      "RECALL: 0.972 (0.025)\n",
      "F1: 0.974 (0.024)\n",
      "PRECISION: 0.978 (0.018)\n",
      "==================================================\n",
      "TEST\n",
      "AUC: 0.929 (0.029)\n",
      "MCC: 0.65 (0.075)\n",
      "ACCURACY: 0.886 (0.034)\n",
      "RECALL: 0.886 (0.034)\n",
      "F1: 0.89 (0.033)\n",
      "PRECISION: 0.897 (0.031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"forward\",\"borutashap\",\"powershap\",\"shapicant\",\"chi\",\"f_test\",\"default\"]:\n",
    "\n",
    "    if model == \"forward\":\n",
    "        selected_features = ['Att240', 'Att200', 'Att88', 'Att46', 'Att214', \n",
    "                   'Att53', 'Att118', 'Att80', 'Att225', 'Att22', 'Att32', 'Att191', 'Att58', 'Att65', 'Att245'] #forward feature selection on AUC\n",
    "\n",
    "    elif model ==\"borutashap\":\n",
    "        selected_features = ['Att83', 'Att240', 'Att226', 'Att223', 'Att45', 'Att98', 'Att89', 'Att241', 'Att222', 'Att245', 'Att82', 'Att72', 'Att22', 'Att91'] #borutoSHAP \n",
    "\n",
    "    elif model ==\"powershap\":\n",
    "        processed_shaps_df = pd.read_csv(\"results/scene_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "        selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "    elif model ==\"shapicant\":\n",
    "        #shapicant\n",
    "        selected_features = ['Att15', 'Att17', 'Att18', 'Att20', 'Att22', 'Att23', 'Att27',\n",
    "           'Att40', 'Att44', 'Att45', 'Att47', 'Att48', 'Att49', 'Att53',\n",
    "           'Att68', 'Att72', 'Att78', 'Att80', 'Att82', 'Att83', 'Att86',\n",
    "           'Att87', 'Att89', 'Att91', 'Att98', 'Att103', 'Att106', 'Att108',\n",
    "           'Att118', 'Att132', 'Att133', 'Att141', 'Att171', 'Att185',\n",
    "           'Att195', 'Att200', 'Att201', 'Att204', 'Att205', 'Att207',\n",
    "           'Att208', 'Att209', 'Att212', 'Att222', 'Att223', 'Att226',\n",
    "           'Att228', 'Att229', 'Att237', 'Att239', 'Att240', 'Att241',\n",
    "           'Att242', 'Att245', 'Att253', 'Att269']\n",
    "\n",
    "    elif model ==\"chi\":\n",
    "        #chi squared p value = 0.01\n",
    "        selected_features = list(current_db_train.columns.values[1:-6][np.where(chi2(current_db_train[current_db_train.columns.values[1:-6]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "    elif model ==\"f_test\":\n",
    "        #f_classif p value = 0.01\n",
    "        selected_features = list(current_db_train.columns.values[1:-6][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-6]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "    elif model ==\"default\":\n",
    "        selected_features = list(current_db_train.columns.values[1:-6])\n",
    "\n",
    "    print(len(selected_features))\n",
    "\n",
    "    CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2,use_best_model=True,\n",
    "                                  class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)])\n",
    "\n",
    "    scores_cv_train,scores_cv_test = benchmark_classification_cross_validation(Model = CB_model,Inp_db = current_db_train.copy(deep=True),index_col=Index_col,folds=10,RS=0,features = selected_features,target_col = target_col)\n",
    "\n",
    "    print(model)\n",
    "    print(\"TRAIN\")\n",
    "    for key in scores_cv_train:\n",
    "        print(str(key)+\": \"+str(np.round(np.mean(scores_cv_train[key]),3))+\" (\"+str(np.round(np.std(scores_cv_train[key]),3))+\")\")\n",
    "    print(50*\"=\")\n",
    "    print(\"TEST\")\n",
    "    for key in scores_cv_test:\n",
    "        print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")\n",
    "    print(100*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:11<00:00, 88.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapicant\n",
      "TEST\n",
      "AUC: 0.905 (0.018)\n",
      "MCC: 0.551 (0.055)\n",
      "ACCURACY: 0.871 (0.017)\n",
      "RECALL: 0.871 (0.017)\n",
      "F1: 0.869 (0.018)\n",
      "PRECISION: 0.869 (0.018)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [\"shapicant\"]:#\"forward\",\"borutashap\",\"powershap\",\"shapicant\",\"chi\",\"f_test\",\"default\"]:\n",
    "\n",
    "\n",
    "    if model == \"forward\":\n",
    "        selected_features = ['Att240', 'Att200', 'Att88', 'Att46', 'Att214', \n",
    "                   'Att53', 'Att118', 'Att80', 'Att225', 'Att22', 'Att32', 'Att191', 'Att58', 'Att65', 'Att245'] #forward feature selection on AUC\n",
    "\n",
    "    elif model ==\"borutashap\":\n",
    "        selected_features = ['Att83', 'Att240', 'Att226', 'Att223', 'Att45', 'Att98', 'Att89', 'Att241', 'Att222', 'Att245', 'Att82', 'Att72', 'Att22', 'Att91'] #borutoSHAP \n",
    "\n",
    "    elif model ==\"powershap\":\n",
    "        #processed_shaps_df = pd.read_csv(\"results/scene_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "        #selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "        selected_features = t[t.p_value<0.01].index.values\n",
    "\n",
    "    elif model ==\"shapicant\":\n",
    "        #shapicant\n",
    "        selected_features = ['Att15', 'Att17', 'Att18', 'Att20', 'Att22', 'Att23', 'Att27',\n",
    "           'Att40', 'Att44', 'Att45', 'Att47', 'Att48', 'Att49', 'Att53',\n",
    "           'Att68', 'Att72', 'Att78', 'Att80', 'Att82', 'Att83', 'Att86',\n",
    "           'Att87', 'Att89', 'Att91', 'Att98', 'Att103', 'Att106', 'Att108',\n",
    "           'Att118', 'Att132', 'Att133', 'Att141', 'Att171', 'Att185',\n",
    "           'Att195', 'Att200', 'Att201', 'Att204', 'Att205', 'Att207',\n",
    "           'Att208', 'Att209', 'Att212', 'Att222', 'Att223', 'Att226',\n",
    "           'Att228', 'Att229', 'Att237', 'Att239', 'Att240', 'Att241',\n",
    "           'Att242', 'Att245', 'Att253', 'Att269']\n",
    "\n",
    "    elif model ==\"chi\":\n",
    "        #chi squared p value = 0.01\n",
    "        selected_features = list(current_db_train.columns.values[1:-6][np.where(chi2(current_db_train[current_db_train.columns.values[1:-6]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "    elif model ==\"f_test\":\n",
    "        #f_classif p value = 0.01\n",
    "        selected_features = list(current_db_train.columns.values[1:-6][np.where(f_classif(current_db_train[current_db_train.columns.values[1:-6]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "    elif model ==\"default\":\n",
    "        selected_features = list(current_db_train.columns.values[1:-6])\n",
    "\n",
    "    print(len(selected_features))\n",
    "\n",
    "    X_train = current_db_train[selected_features]\n",
    "    Y_train = current_db_train[target_col]\n",
    "\n",
    "    X_test = current_db_test[selected_features]\n",
    "    Y_test = current_db_test[target_col]\n",
    "\n",
    "    CB_model = CatBoostClassifier(verbose=False,iterations=250,random_seed=2,\n",
    "                                  class_weights=[1-len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train),len(current_db_train[current_db_train[target_col] == 0])/len(current_db_train)])\n",
    "    CB_model.fit(X_train,Y_train)\n",
    "\n",
    "    scores_cv_test = test_bootstrap_eval_class(Model = CB_model,test_df = current_db_test.copy(deep=True),bootstrap_its=1000,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "    print(model)\n",
    "    print(\"TEST\")\n",
    "    for key in scores_cv_test:\n",
    "        print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")\n",
    "        \n",
    "    print(100*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT Location Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db = pd.read_csv(\"data/slice_localization_data.csv\")\n",
    "current_db = current_db.reset_index()\n",
    "\n",
    "Index_col = \"patientId\"\n",
    "target_col = \"reference\"\n",
    "\n",
    "train_idx,val_idx = train_test_split(current_db[Index_col].unique(),test_size=0.25,random_state = 1)\n",
    "current_db_train = current_db[current_db[Index_col].isin(train_idx)]\n",
    "current_db_test = current_db[current_db[Index_col].isin(val_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### powershap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting powershap\n",
      "Automatic mode enabled: Finding the minimal required powershap iterations for significance of 0.01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599b3a0eb6e34a06a26563e438eaab4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 41  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d33cc7bc85442c1a4a7c0fcb6372358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 57  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1a605d095c44b1a9f341bb1cbf2047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automatic mode: powershap requires 62  iterations; The extra required iterations exceed the limit_automatic  threshold. Powershap will add  10 powershap iterations and  re-evaluate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dc88052f834ca2b8a67b57da0167e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "selector = PowerSHAP(\n",
    "    model = CatBoostRegressor(verbose=0, n_estimators=250,use_best_model=True),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,target_col=target_col,index_col=Index_col,\n",
    ")\n",
    "selector.fit(current_db_train[list(current_db_train.columns.values[2:-1])], current_db_train[target_col])\n",
    "t = selector._processed_shaps_df\n",
    "#t.reset_index().to_csv(\"results/CT_location_PowerSHAP_catboost_results_automatic_mode.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borutashap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c45dda19a45878ff5db0d85a22d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "162 attributes confirmed important: ['value53', 'value25', 'value145', 'value116', 'value94', 'value190', 'value21', 'value23', 'value83', 'value210', 'value118', 'value273', 'value282', 'value122', 'value207', 'value2', 'value226', 'value242', 'value338', 'value52', 'value132', 'value320', 'value150', 'value140', 'value126', 'value146', 'value124', 'value236', 'value220', 'value131', 'value4', 'value105', 'value138', 'value248', 'value35', 'value258', 'value120', 'value100', 'value231', 'value265', 'value134', 'value378', 'value26', 'value215', 'value237', 'value90', 'value72', 'value117', 'value222', 'value223', 'value160', 'value181', 'value339', 'value110', 'value81', 'value307', 'value152', 'value201', 'value291', 'value33', 'value141', 'value211', 'value84', 'value6', 'value216', 'value252', 'value173', 'value112', 'value114', 'value234', 'value280', 'value61', 'value221', 'value171', 'value130', 'value241', 'value63', 'value95', 'value91', 'value13', 'value143', 'value218', 'value266', 'value133', 'value256', 'value306', 'value135', 'value123', 'value212', 'value281', 'value142', 'value151', 'value228', 'value232', 'value14', 'value96', 'value106', 'value235', 'value5', 'value113', 'value85', 'value300', 'value318', 'value213', 'value292', 'value7', 'value172', 'value191', 'value272', 'value224', 'value276', 'value125', 'value238', 'value264', 'value251', 'value275', 'value18', 'value182', 'value298', 'value362', 'value111', 'value230', 'value246', 'value30', 'value200', 'value101', 'value127', 'value136', 'value305', 'value283', 'value108', 'value8', 'value0', 'value183', 'value22', 'value314', 'value115', 'value377', 'value170', 'value60', 'value382', 'value227', 'value16', 'value104', 'value64', 'value299', 'value121', 'value3', 'value214', 'value308', 'value322', 'value174', 'value331', 'value54', 'value233', 'value274', 'value44', 'value370', 'value180', 'value225', 'value102', 'value192']\n",
      "48 attributes confirmed unimportant: ['value97', 'value69', 'value326', 'value58', 'value75', 'value59', 'value78', 'value48', 'value49', 'value271', 'value188', 'value179', 'value79', 'value350', 'value317', 'value325', 'value303', 'value254', 'value178', 'value189', 'value253', 'value301', 'value375', 'value109', 'value199', 'value279', 'value359', 'value177', 'value156', 'value169', 'value255', 'value198', 'value217', 'value278', 'value315', 'value343', 'value287', 'value68', 'value193', 'value340', 'value302', 'value293', 'value27', 'value351', 'value168', 'value374', 'value353', 'value347']\n",
      "174 tentative attributes remains: ['value323', 'value77', 'value128', 'value357', 'value129', 'value57', 'value163', 'value261', 'value285', 'value10', 'value277', 'value270', 'value365', 'value319', 'value202', 'value250', 'value206', 'value267', 'value93', 'value149', 'value29', 'value56', 'value195', 'value316', 'value40', 'value245', 'value9', 'value328', 'value87', 'value99', 'value187', 'value335', 'value349', 'value380', 'value376', 'value45', 'value197', 'value358', 'value310', 'value332', 'value55', 'value185', 'value371', 'value355', 'value205', 'value381', 'value296', 'value107', 'value368', 'value342', 'value62', 'value290', 'value12', 'value321', 'value334', 'value42', 'value24', 'value297', 'value157', 'value31', 'value167', 'value259', 'value333', 'value38', 'value80', 'value98', 'value336', 'value337', 'value366', 'value348', 'value88', 'value71', 'value361', 'value20', 'value209', 'value263', 'value144', 'value345', 'value28', 'value295', 'value344', 'value269', 'value32', 'value19', 'value304', 'value312', 'value363', 'value383', 'value82', 'value186', 'value39', 'value243', 'value324', 'value352', 'value219', 'value70', 'value67', 'value356', 'value165', 'value37', 'value154', 'value184', 'value153', 'value76', 'value341', 'value17', 'value313', 'value147', 'value367', 'value288', 'value139', 'value148', 'value294', 'value15', 'value159', 'value158', 'value289', 'value11', 'value155', 'value51', 'value50', 'value43', 'value229', 'value194', 'value244', 'value369', 'value327', 'value92', 'value73', 'value284', 'value249', 'value260', 'value247', 'value46', 'value372', 'value166', 'value103', 'value36', 'value74', 'value329', 'value175', 'value41', 'value164', 'value137', 'value161', 'value257', 'value176', 'value360', 'value162', 'value354', 'value286', 'value240', 'value1', 'value268', 'value119', 'value203', 'value65', 'value208', 'value379', 'value330', 'value196', 'value346', 'value204', 'value239', 'value86', 'value89', 'value262', 'value47', 'value34', 'value373', 'value66', 'value311', 'value309', 'value364']\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostRegressor(verbose=False,iterations=250)\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=False)\n",
    "\n",
    "Feature_Selector.fit(X=current_db_train[list(current_db_train.columns.values[2:-1])], y=current_db_train[target_col], sample=False,\n",
    "                        train_or_test = 'test', normalize=True,verbose=True)\n",
    "subset = Feature_Selector.Subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [25:53<00:00, 15.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['value0', 'value2', 'value3', 'value4', 'value5', 'value8',\n",
       "       'value18', 'value28', 'value29', 'value38', 'value47', 'value53',\n",
       "       'value55', 'value60', 'value63', 'value88', 'value106', 'value108',\n",
       "       'value110', 'value114', 'value115', 'value116', 'value118',\n",
       "       'value132', 'value135', 'value136', 'value137', 'value138',\n",
       "       'value145', 'value150', 'value167', 'value170', 'value172',\n",
       "       'value182', 'value183', 'value197', 'value200', 'value209',\n",
       "       'value210', 'value212', 'value213', 'value215', 'value225',\n",
       "       'value226', 'value227', 'value228', 'value230', 'value233',\n",
       "       'value237', 'value238', 'value241', 'value251', 'value264',\n",
       "       'value269', 'value270', 'value272', 'value273', 'value280',\n",
       "       'value291', 'value295', 'value300', 'value310', 'value311',\n",
       "       'value318', 'value319', 'value322', 'value334', 'value338',\n",
       "       'value341', 'value367', 'value370', 'value377', 'value378',\n",
       "       'value382'], dtype='<U8')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inp_db = current_db_train.copy(deep=True)\n",
    "train_idx,val_idx = train_test_split(Inp_db[Index_col],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True)[list(current_db_train.columns.values[2:-1])]\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True)[list(current_db_train.columns.values[2:-1])]\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostRegressor(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_model = CatBoostRegressor(verbose=False,iterations=250,use_best_model=True)\n",
    "CV_features_arr_final,Metrics_best = regression_forward_feature_selection(shap_model,current_db_train,Index_col,5,0,current_db_train.drop(columns=[target_col,Index_col]).columns.values,target_col,\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:52,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TRAIN\n",
      "R2: 0.991 (0.001)\n",
      "RMSE: 2.111 (0.139)\n",
      "MAE: 1.49 (0.092)\n",
      "ME: 0.0 (0.0)\n",
      "==================================================\n",
      "TEST\n",
      "R2: 0.917 (0.03)\n",
      "RMSE: 6.146 (1.191)\n",
      "MAE: 3.882 (0.609)\n",
      "ME: -0.357 (0.901)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"powershap\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['value237', 'value378', 'value114', 'value273', 'value172', 'value170',\n",
    "               'value3', 'value116', 'value291', 'value18', 'value226', 'value238', 'value53', 'value142', 'value194', 'value370', \n",
    "               'value299', 'value120', 'value35', 'value10', 'value264', 'value200', 'value316', 'value135', 'value13'] \n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['value53', 'value25', 'value145', 'value116', 'value94', 'value190', 'value21', 'value23', 'value83', \n",
    "                         'value210', 'value118', 'value273', 'value282', 'value122', 'value207', 'value2', 'value226', 'value242', \n",
    "                         'value338', 'value52', 'value132', 'value320', 'value150', 'value140', 'value126', 'value146', 'value124', \n",
    "                         'value236', 'value220', 'value131', 'value4', 'value105', 'value138', 'value248', 'value35', 'value258', \n",
    "                         'value120', 'value100', 'value231', 'value265', 'value134', 'value378', 'value26', 'value215', 'value237', \n",
    "                         'value90', 'value72', 'value117', 'value222', 'value223', 'value160', 'value181', 'value339', 'value110', \n",
    "                         'value81', 'value307', 'value152', 'value201', 'value291', 'value33', 'value141', 'value211', 'value84', \n",
    "                         'value6', 'value216', 'value252', 'value173', 'value112', 'value114', 'value234', 'value280', 'value61', \n",
    "                         'value221', 'value171', 'value130', 'value241', 'value63', 'value95', 'value91', 'value13', 'value143', \n",
    "                         'value218', 'value266', 'value133', 'value256', 'value306', 'value135', 'value123', 'value212', 'value281', \n",
    "                         'value142', 'value151', 'value228', 'value232', 'value14', 'value96', 'value106', 'value235', 'value5', \n",
    "                         'value113', 'value85', 'value300', 'value318', 'value213', 'value292', 'value7', 'value172', 'value191', \n",
    "                         'value272', 'value224', 'value276', 'value125', 'value238', 'value264', 'value251', 'value275', 'value18', \n",
    "                         'value182', 'value298', 'value362', 'value111', 'value230', 'value246', 'value30', 'value200', 'value101', \n",
    "                         'value127', 'value136', 'value305', 'value283', 'value108', 'value8', 'value0', 'value183', 'value22', \n",
    "                         'value314', 'value115', 'value377', 'value170', 'value60', 'value382', 'value227', 'value16', 'value104', \n",
    "                         'value64', 'value299', 'value121', 'value3', 'value214', 'value308', 'value322', 'value174', 'value331', \n",
    "                         'value54', 'value233', 'value274', 'value44', 'value370', 'value180', 'value225', 'value102', 'value192']\n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    #processed_shaps_df = pd.read_csv(\"results/CT_location_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    selected_features = t[t.p_value<0.01].index.values#processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['value0', 'value2', 'value3', 'value4', 'value5', 'value8',\n",
    "       'value18', 'value28', 'value29', 'value38', 'value47', 'value53',\n",
    "       'value55', 'value60', 'value63', 'value88', 'value106', 'value108',\n",
    "       'value110', 'value114', 'value115', 'value116', 'value118',\n",
    "       'value132', 'value135', 'value136', 'value137', 'value138',\n",
    "       'value145', 'value150', 'value167', 'value170', 'value172',\n",
    "       'value182', 'value183', 'value197', 'value200', 'value209',\n",
    "       'value210', 'value212', 'value213', 'value215', 'value225',\n",
    "       'value226', 'value227', 'value228', 'value230', 'value233',\n",
    "       'value237', 'value238', 'value241', 'value251', 'value264',\n",
    "       'value269', 'value270', 'value272', 'value273', 'value280',\n",
    "       'value291', 'value295', 'value300', 'value310', 'value311',\n",
    "       'value318', 'value319', 'value322', 'value334', 'value338',\n",
    "       'value341', 'value367', 'value370', 'value377', 'value378',\n",
    "       'value382']\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[2:-1][np.where(f_regression(current_db_train[current_db_train.columns.values[2:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[2:-1])\n",
    "\n",
    "print(len(selected_features))\n",
    "\n",
    "CB_model = CatBoostRegressor(verbose=False,iterations=250,random_seed=2,use_best_model=True)\n",
    "\n",
    "scores_cv_train,scores_cv_test = benchmark_regression_cross_validation(Model = CB_model,Inp_db = current_db_train.copy(deep=True),index_col=Index_col,folds=10,RS=0,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TRAIN\")\n",
    "for key in scores_cv_train:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_train[key]),3))+\" (\"+str(np.round(np.std(scores_cv_train[key]),3))+\")\")\n",
    "print(50*\"=\")\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.217117\n",
      "0:\tlearn: 18.7696802\ttotal: 54.5ms\tremaining: 13.6s\n",
      "100:\tlearn: 3.2398502\ttotal: 2.07s\tremaining: 3.06s\n",
      "200:\tlearn: 2.3530439\ttotal: 4.06s\tremaining: 989ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                               | 3/100 [00:00<00:04, 23.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249:\tlearn: 2.1444204\ttotal: 5s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powershap\n",
      "TEST\n",
      "R2: 0.886 (0.004)\n",
      "RMSE: 7.161 (0.13)\n",
      "MAE: 4.347 (0.061)\n",
      "ME: -0.054 (0.083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"powershap\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['value237', 'value378', 'value114', 'value273', 'value172', 'value170',\n",
    "               'value3', 'value116', 'value291', 'value18', 'value226', 'value238', 'value53', 'value142', 'value194', 'value370', \n",
    "               'value299', 'value120', 'value35', 'value10', 'value264', 'value200', 'value316', 'value135', 'value13'] \n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['value2', 'value171', 'value32', 'value141', 'value252', 'value192', 'value133', 'value8', 'value155', \n",
    "                         'value265', 'value135', 'value223', 'value292', 'value290', 'value151', 'value112', 'value377', 'value233', \n",
    "                         'value26', 'value220', 'value30', 'value140', 'value230', 'value248', 'value211', 'value85', 'value172', \n",
    "                         'value221', 'value22', 'value145', 'value331', 'value251', 'value3', 'value222', 'value131', 'value13', \n",
    "                         'value160', 'value370', 'value114', 'value228', 'value276', 'value16', 'value0', 'value111', 'value117', \n",
    "                         'value280', 'value104', 'value154', 'value273', 'value134', 'value237', 'value224', 'value212', 'value5', \n",
    "                         'value83', 'value116', 'value184', 'value120', 'value182', 'value136', 'value242', 'value235', 'value267', \n",
    "                         'value190', 'value215', 'value339', 'value84', 'value371', 'value14', 'value241', 'value35', 'value214', \n",
    "                         'value298', 'value61', 'value299', 'value275', 'value300', 'value110', 'value281', 'value291', 'value161', \n",
    "                         'value274', 'value362', 'value201', 'value308', 'value91', 'value4', 'value53', 'value81', 'value34', \n",
    "                         'value103', 'value183', 'value207', 'value174', 'value283', 'value226', 'value52', 'value122', 'value258', \n",
    "                         'value146', 'value150', 'value127', 'value288', 'value92', 'value105', 'value232', 'value236', 'value101', \n",
    "                         'value225', 'value6', 'value227', 'value170', 'value216', 'value118', 'value64', 'value191', 'value180', \n",
    "                         'value7', 'value256', 'value113', 'value213', 'value259', 'value63', 'value132', 'value123', 'value312', \n",
    "                         'value181', 'value138', 'value378', 'value200', 'value210', 'value125', 'value369', 'value106', 'value264', \n",
    "                         'value90', 'value282', 'value307', 'value130', 'value219', 'value152', 'value126', 'value244', \n",
    "                         'value173', 'value142', 'value143', 'value124', 'value60', 'value257', 'value234', \n",
    "                         'value272', 'value115', 'value108', 'value266', 'value18', 'value44', 'value33', 'value314', 'value100', 'value42', 'value231', 'value260', 'value320']\n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    #processed_shaps_df = pd.read_csv(\"results/CT_location_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    #selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "    selected_features = t[t.p_value<0.01].index.values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['value0', 'value2', 'value3', 'value4', 'value5', 'value8',\n",
    "       'value18', 'value28', 'value29', 'value38', 'value47', 'value53',\n",
    "       'value55', 'value60', 'value63', 'value88', 'value106', 'value108',\n",
    "       'value110', 'value114', 'value115', 'value116', 'value118',\n",
    "       'value132', 'value135', 'value136', 'value137', 'value138',\n",
    "       'value145', 'value150', 'value167', 'value170', 'value172',\n",
    "       'value182', 'value183', 'value197', 'value200', 'value209',\n",
    "       'value210', 'value212', 'value213', 'value215', 'value225',\n",
    "       'value226', 'value227', 'value228', 'value230', 'value233',\n",
    "       'value237', 'value238', 'value241', 'value251', 'value264',\n",
    "       'value269', 'value270', 'value272', 'value273', 'value280',\n",
    "       'value291', 'value295', 'value300', 'value310', 'value311',\n",
    "       'value318', 'value319', 'value322', 'value334', 'value338',\n",
    "       'value341', 'value367', 'value370', 'value377', 'value378',\n",
    "       'value382']\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[2:-1][np.where(f_regression(current_db_train[current_db_train.columns.values[2:-1]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[2:-1])\n",
    "\n",
    "X_train = current_db_train[selected_features]\n",
    "Y_train = current_db_train[target_col]\n",
    "\n",
    "X_test = current_db_test[selected_features]\n",
    "Y_test = current_db_test[target_col]\n",
    "\n",
    "CB_model = CatBoostRegressor(verbose=100,iterations=250,random_seed=2)#,per_float_feature_quantization=['1:border_count=1024'])\n",
    "CB_model.fit(X_train,Y_train)\n",
    "        \n",
    "scores_cv_test = test_bootstrap_eval_regres(Model = CB_model,test_df = current_db_test.copy(deep=True),bootstrap_its=100,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appliances Energy Production Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db = pd.read_csv(\"data/energydata_complete.csv\")\n",
    "current_db = current_db.reset_index()\n",
    "\n",
    "Index_col = \"index\"\n",
    "target_col = \"Appliances\"\n",
    "\n",
    "train_idx,val_idx = train_test_split(current_db[Index_col],test_size=0.25,random_state = 1)\n",
    "current_db_train = current_db[current_db[Index_col].isin(train_idx)]\n",
    "current_db_test = current_db[current_db[Index_col].isin(val_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### powershap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = PowerSHAP(\n",
    "    model = CatBoostRegressor(verbose=0, n_estimators=250,use_best_model=True),\n",
    "    power_iterations=10,automatic=True, limit_automatic=10,verbose=True,target_col=target_col,index_col=Index_col,\n",
    ")\n",
    "selector.fit(current_db_train[list(current_db_train.columns.values[3:])], current_db_train[target_col])\n",
    "t = selector._processed_shaps_df\n",
    "t.reset_index().to_csv(\"results/appliances_PowerSHAP_catboost_results_automatic_mode.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### borutashap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(verbose=False,iterations=250)\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=False)\n",
    "\n",
    "Feature_Selector.fit(X=current_db_train[list(current_db_train.columns.values[3:])], y=current_db_train[target_col], sample=False,\n",
    "                        train_or_test = 'test', normalize=True,verbose=True)\n",
    "subset = Feature_Selector.Subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shapicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing null SHAP values: 100%|████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['lights', 'T1', 'RH_1', 'RH_2', 'T3', 'RH_3', 'RH_6', 'RH_8',\n",
       "       'Press_mm_hg', 'Windspeed'], dtype='<U11')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inp_db = current_db_train.copy(deep=True)\n",
    "train_idx,val_idx = train_test_split(Inp_db[Index_col],test_size=0.2,random_state = 0)\n",
    "\n",
    "X_train = Inp_db[Inp_db[Index_col].isin(train_idx)].copy(deep=True)[list(current_db_train.columns.values[3:])]\n",
    "X_val = Inp_db[Inp_db[Index_col].isin(val_idx)].copy(deep=True)[list(current_db_train.columns.values[3:])]\n",
    "Y_train = Inp_db[Inp_db[Index_col].isin(train_idx)][target_col]\n",
    "\n",
    "# LightGBM in RandomForest-like mode (with rows subsampling), without columns subsampling\n",
    "model = CatBoostRegressor(verbose=False,iterations=250,use_best_model=False)\n",
    "\n",
    "# This is the class (not its instance) of SHAP's TreeExplainer\n",
    "explainer_type = shap.TreeExplainer\n",
    "\n",
    "# Use PandasSelector with 100 iterations\n",
    "selector = shapicant.PandasSelector(model, explainer_type, random_state=42)\n",
    "\n",
    "# Run the feature selection\n",
    "# If we provide a validation set, SHAP values are computed on it, otherwise they are computed on the training set\n",
    "# We can also provide additional parameters to the underlying estimator's fit method through estimator_params\n",
    "selector.fit(X_train, Y_train, X_validation=X_val)#, estimator_params={\"categorical_feature\": None})\n",
    "\n",
    "# Just get the features list\n",
    "selected_features = selector.get_features()\n",
    "\n",
    "# We can also get the p-values as pandas Series\n",
    "p_values = selector.p_values_\n",
    "\n",
    "np.array(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_model = CatBoostRegressor(verbose=False,iterations=250,use_best_model=True)\n",
    "CV_features_arr_final,Metrics_best = regression_forward_feature_selection(shap_model,current_db_train,Index_col,5,0,current_db_train.columns.values[3:],target_col,\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:07,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "R2: 0.601 (0.007)\n",
      "RMSE: 64.322 (0.496)\n",
      "MAE: 34.978 (0.255)\n",
      "ME: -0.042 (0.007)\n",
      "==================================================\n",
      "TEST\n",
      "R2: 0.375 (0.019)\n",
      "RMSE: 80.394 (4.129)\n",
      "MAE: 41.756 (1.79)\n",
      "ME: -0.5 (1.891)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"shapicant\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['lights', 'T9', 'Press_mm_hg', 'T_out', 'RH_2', 'T4', 'T8', 'RH_8', 'RH_5', 'RH_4', 'T7', 'Tdewpoint', 'T6']\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['RH_3', 'T5', 'RH_8', 'T4', 'Tdewpoint', 'T3', 'lights', 'RH_1', \n",
    "                         'T6', 'T2', 'RH_2', 'RH_4', 'T9', 'RH_5', 'RH_7', 'T8', 'RH_9', 'T7', 'RH_6', 'T_out', 'T1', 'Press_mm_hg', 'RH_out', 'Windspeed']\n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    processed_shaps_df = pd.read_csv(\"results/appliances_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['lights', 'T1', 'RH_1', 'RH_2', 'T3', 'RH_3', 'RH_6', 'RH_8',\n",
    "       'Press_mm_hg', 'Windspeed']\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[3:][np.where(f_regression(current_db_train[current_db_train.columns.values[3:]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[3:])\n",
    "\n",
    "print(len(selected_features))\n",
    "\n",
    "CB_model = CatBoostRegressor(verbose=False,iterations=250,random_seed=2)\n",
    "\n",
    "scores_cv_train,scores_cv_test = benchmark_regression_cross_validation(Model = CB_model,Inp_db = current_db_train.copy(deep=True),index_col=Index_col,folds=10,RS=0,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TRAIN\")\n",
    "for key in scores_cv_train:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_train[key]),3))+\" (\"+str(np.round(np.std(scores_cv_train[key]),3))+\")\")\n",
    "print(50*\"=\")\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:09<00:00, 107.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "R2: 0.388 (0.02)\n",
      "RMSE: 81.663 (2.891)\n",
      "MAE: 42.569 (1.162)\n",
      "ME: -2.309 (1.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"shapicant\"\n",
    "\n",
    "if model == \"forward\":\n",
    "    selected_features = ['lights', 'T9', 'Press_mm_hg', 'T_out', 'RH_2', 'T4', 'T8', 'RH_8', 'RH_5', 'RH_4', 'T7', 'Tdewpoint', 'T6']\n",
    "    \n",
    "elif model ==\"borutashap\":\n",
    "    selected_features = ['RH_3', 'T5', 'RH_8', 'T4', 'Tdewpoint', 'T3', 'lights', 'RH_1', \n",
    "                         'T6', 'T2', 'RH_2', 'RH_4', 'T9', 'RH_5', 'RH_7', 'T8', 'RH_9', 'T7', 'RH_6', 'T_out', 'T1', 'Press_mm_hg', 'RH_out', 'Windspeed']\n",
    "    \n",
    "elif model ==\"powershap\":\n",
    "    processed_shaps_df = pd.read_csv(\"results/appliances_PowerSHAP_catboost_results_automatic_mode.csv\")\n",
    "    selected_features = processed_shaps_df[(processed_shaps_df.p_value<0.01)][\"index\"].values\n",
    "\n",
    "elif model ==\"shapicant\":\n",
    "    #shapicant\n",
    "    selected_features = ['lights', 'T1', 'RH_1', 'RH_2', 'T3', 'RH_3', 'RH_6', 'RH_8',\n",
    "       'Press_mm_hg', 'Windspeed']\n",
    "\n",
    "elif model ==\"f_test\":\n",
    "    #f_classif p value = 0.01\n",
    "    selected_features = list(current_db_train.columns.values[3:][np.where(f_regression(current_db_train[current_db_train.columns.values[3:]],current_db_train[target_col])[1]<0.01)[0]])\n",
    "\n",
    "elif model ==\"default\":\n",
    "    selected_features = list(current_db_train.columns.values[3:])\n",
    "\n",
    "print(len(selected_features))\n",
    "\n",
    "X_train = current_db_train[selected_features]\n",
    "Y_train = current_db_train[target_col]\n",
    "\n",
    "X_test = current_db_test[selected_features]\n",
    "Y_test = current_db_test[target_col]\n",
    "\n",
    "CB_model = CatBoostRegressor(verbose=False,iterations=250,random_seed=2)#,per_float_feature_quantization=['1:border_count=1024'])\n",
    "CB_model.fit(X_train,Y_train)\n",
    "        \n",
    "scores_cv_test = test_bootstrap_eval_regres(Model = CB_model,test_df = current_db_test.copy(deep=True),bootstrap_its=1000,RS=1,features = selected_features,target_col = target_col)\n",
    "\n",
    "print(model)\n",
    "print(\"TEST\")\n",
    "for key in scores_cv_test:\n",
    "    print(str(key)+\": \"+str(np.round(np.mean(scores_cv_test[key]),3))+\" (\"+str(np.round(np.std(scores_cv_test[key]),3))+\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
